{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6e8b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Model is on: mps:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/v5tsk2x9565b7d_bmh7_cpdc0000gn/T/ipykernel_4244/235889164.py:87: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/health-hw7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 09:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.963700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/health-hw7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/health-hw7/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ‚úÖ Make sure we‚Äôre on MPS\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# üß† Load base model and tokenizer\n",
    "model_name = \"google/gemma-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32, cache_dir=\"./hf_cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"./hf_cache\")\n",
    "model = get_peft_model(model, LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    "))\n",
    "\n",
    "# üöö Move model to MPS\n",
    "model.to(device)\n",
    "print(f\"üß† Model is on: {next(model.parameters()).device}\")\n",
    "\n",
    "# üìö Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"deid_dataset.jsonl\", split=\"train\")\n",
    "\n",
    "# üîß Format prompt\n",
    "def format_example(example):\n",
    "    prompt = (\n",
    "        \"You are a medical assistant. Given a clinical note containing sensitive patient information (PHI), your job is to:\\n\"\n",
    "        \"1. Identify all instances of PHI.\\n\"\n",
    "        \"2. Reason through what should be redacted and why.\\n\"\n",
    "        \"3. Output the redacted note, replacing PHI with [REDACTED] or placeholders like [DOB], [NAME], [ADDRESS].\\n\"\n",
    "        \"Note: PHI includes names, dates of birth, phone numbers, SSNs, addresses, provider names, hospitals, emails, etc.\\n\"\n",
    "        \"---\\n\"\n",
    "        f\"<data_with_phi>\\n{example['data_with_phi']}\\n</data_with_phi>\\n\"\n",
    "        f\"<data_hipaa_compliant>\\n{example['data_hipaa_compliant']}\\n</data_hipaa_compliant>\"\n",
    "    )\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# üîÑ Preprocess\n",
    "dataset = dataset.map(format_example, remove_columns=dataset.column_names)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ü§ñ Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# üõ†Ô∏è Training args ‚Äî NO fp16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-deid-lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=False,           # üîí Explicitly disable mixed precision\n",
    "    bf16=False,           # üîí Also disable bf16 just in case\n",
    "    torch_compile=False,  # üîí Avoid weird compile errors on MPS\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# üîÅ Resume logic\n",
    "checkpoint_dir = training_args.output_dir\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(checkpoint_dir):\n",
    "    subdirs = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint\")]\n",
    "    if subdirs:\n",
    "        last_checkpoint = os.path.join(checkpoint_dir, sorted(subdirs)[-1])\n",
    "        print(f\"üîÅ Resuming from checkpoint: {last_checkpoint}\")\n",
    "\n",
    "# üèãÔ∏è Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# üöÄ Train\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0af4820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.82it/s]\n",
      "/opt/anaconda3/envs/health-hw7/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Redacted note:\n",
      " Patient: Ms. Yaeko Ming [REDACTED], SSN: [REDACTED], born on [REDACTED] in [REDACTED], presented to [REDACTED] on [REDACTED] for an encounter for problem [REDACTED] related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. [REDACTED], general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at [REDACTED], Cando, ND‚ÄØ58324 (Towner County, FIPS‚ÄØ38095), and is a white, non‚ÄëHispanic female with no recorded marital status.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# üîß Paths to model\n",
    "BASE_MODEL = \"google/gemma-2b\"\n",
    "LORA_MODEL_PATH = \"./gemma-deid-lora/checkpoint-60\"\n",
    "\n",
    "# üîÅ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# üß† Load base model and merge LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "\n",
    "# üíª Use MPS if available on Mac, else CUDA, else CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# üß™ Inference function\n",
    "def redact_note(phi_note: str):\n",
    "    prompt = f\"\"\"You are a medical assistant. Given a clinical note containing sensitive patient information (PHI), your job is to:\n",
    "\n",
    "1. Identify all instances of PHI.\n",
    "2. Reason through what should be redacted and why.\n",
    "3. Output the redacted note, replacing PHI with [REDACTED].\n",
    "\n",
    "PHI includes: names, dates of birth, phone numbers, SSNs, addresses, provider names, hospitals, emails, etc.\n",
    "\n",
    "---\n",
    "<data_with_phi>\n",
    "{phi_note}\n",
    "</data_with_phi>\n",
    "<data_hipaa_compliant>\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=300,         # ‚¨ÜÔ∏è Allow longer outputs\n",
    "            temperature=0.7,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id  # üëà Avoid warnings\n",
    "            # eos_token_id removed to prevent premature stopping\n",
    "        )\n",
    "\n",
    "    # Decode and extract only redacted portion\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"<data_hipaa_compliant>\" in decoded:\n",
    "        redacted_part = decoded.split(\"<data_hipaa_compliant>\")[-1]\n",
    "        redacted_part = redacted_part.split(\"</data_hipaa_compliant>\")[0].strip()\n",
    "    else:\n",
    "        redacted_part = decoded[len(prompt):].strip()\n",
    "\n",
    "    return redacted_part\n",
    "\n",
    "# üîç Example usage\n",
    "phi_input = \"\"\"\n",
    "Patient: Ms. Yaeko Ming Kshlerin, SSN: 999-26-7676, born on 1999‚Äë06‚Äë07 in Oakes, North Dakota, presented to TOWNER COUNTY MEDICAL CENTER INC (HWY‚ÄØ281N, CANDO, ND‚ÄØ58324) on 2000‚Äë11‚Äë20 for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. Shiloh Larson, general practice.  \n",
    "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
    "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, ND‚ÄØ58324 (Towner County, FIPS‚ÄØ38095), and is a white, non‚ÄëHispanic female with no recorded marital status.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì§ Redacted note:\\n\", redact_note(phi_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health-hw7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
