{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df3e48d",
   "metadata": {},
   "source": [
    "# Privacy-Preserving RAG: Clinical Assistant with Tool Calling\n",
    "\n",
    "This notebook implements a clinical assistant powered by OpenAI's GPT4.1 model that can answer patient-specific questions using structured data while preserving privacy through local de-identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3962aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_note_auto(table_name, patient_id):\n",
    "    \"\"\"\n",
    "    Reads the generated *_notes.csv file for the given table and returns \n",
    "    the first clinical note found for the provided patient_id.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: str (e.g., \"observations\", \"medications\")\n",
    "    - patient_id: str (the patient_id to search for)\n",
    "\n",
    "    Returns:\n",
    "    - str: the clinical note or a message if not found\n",
    "    \"\"\"\n",
    "    notes_file = os.path.join(\"data\", f\"{table_name}_notes.csv\")\n",
    "\n",
    "    if not os.path.exists(notes_file):\n",
    "        return f\"Note file not found: {notes_file}\"\n",
    "\n",
    "    df = pd.read_csv(notes_file)\n",
    "    \n",
    "    # Normalize column names just in case\n",
    "    df.columns = [col.strip().lower() for col in df.columns]\n",
    "\n",
    "    if \"patient_id\" not in df.columns or \"clinical_note\" not in df.columns:\n",
    "        return \"Expected columns not found in the CSV.\"\n",
    "\n",
    "    matching_notes = df[df[\"patient_id\"] == patient_id]\n",
    "\n",
    "    if matching_notes.empty:\n",
    "        return f\"No note found for patient_id: {patient_id}\"\n",
    "\n",
    "    return matching_notes.iloc[0][\"clinical_note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978f1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (1.98.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.13/site-packages (4.55.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.13/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.13/site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->transformers) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in ./venv/lib/python3.13/site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.13/site-packages (from peft) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.13/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.13/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.13/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./venv/lib/python3.13/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.13/site-packages (from peft) (4.55.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.13/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./venv/lib/python3.13/site-packages (from peft) (1.9.0)\n",
      "Requirement already satisfied: safetensors in ./venv/lib/python3.13/site-packages (from peft) (0.6.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in ./venv/lib/python3.13/site-packages (from peft) (0.34.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2025.7.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.13/site-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.13/site-packages (from transformers->peft) (0.21.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install essential packages\n",
    "# Run this cell first to install core dependencies\n",
    "\n",
    "!pip install openai python-dotenv pandas numpy\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3be14cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "ðŸ“… Current date: 2025-08-09 16:13:37\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26651648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.13/site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: llama-cpp-python in ./venv/lib/python3.13/site-packages (0.3.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (2.3.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2) - 10916 MiB free\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /Users/arunjoshi/.cache/huggingface/hub/models--google--gemma-3-4b-it-qat-q4_0-gguf/snapshots/15f73f5eee9c28f53afefef5723e29680c2fc78a/./gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
      "llama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\n",
      "llama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\n",
      "llama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\n",
      "llama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\n",
      "llama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\n",
      "llama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\n",
      "llama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\n",
      "llama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\n",
      "llama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\n",
      "llama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\n",
      "llama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\n",
      "llama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - type  f32:  205 tensors\n",
      "llama_model_loader: - type  f16:    1 tensors\n",
      "llama_model_loader: - type q4_0:  238 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 2.93 GiB (6.49 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token: 255999 '<start_of_image>' is not marked as EOG\n",
      "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token: 256000 '<end_of_image>' is not marked as EOG\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 1.9446 MB\n",
      "print_info: arch             = gemma3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 34\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 1024\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 10240\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 0.125\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 4B\n",
      "print_info: model params     = 3.88 B\n",
      "print_info: general.name     = n/a\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 262144\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 106 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 248 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 106 '<end_of_turn>'\n",
      "print_info: max token length = 93\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 206 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/35 layers to GPU\n",
      "load_tensors:   CPU_REPACK model buffer size =  1721.25 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  3002.65 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_down.weight with q4_0_4x8\n",
      "...\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 0.125\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x11f2134a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x11f2136d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x10dd69630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x11f213900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x11f213b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x11f213d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x11f213f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x10c8a3f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x11f213230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x11f646100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x11f646330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x11f646a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x11f646cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10c8a4230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x11f646fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x11f2144a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x10dd69860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x10dd69a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x11f2146d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x10c8a4460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x10c8a4690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x10c8a48c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x10c8a4af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c8a3bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x10dd69cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x11f6471e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x11f2141c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x10c897d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x11f647410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x10dd69140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x10dd6a8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x11f647670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x10dd64180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x11f647ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10dd643b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x11f647ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f647f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f214ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f648140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x10dd69370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x11f648370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10dd646e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f6485a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f214d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f6487d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10dd64a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10dd64c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f648a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f648c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f648e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f649150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10dd64e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f6493e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f649820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10dd650a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f649a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10dd652d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10dd65500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c897f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c8981b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x11f214f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x11f215160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x10c8983e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x11f215390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x10dd65730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x11f2155c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x11f649c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x10c898610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x10c898840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x10c898a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x10c898ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x11f649eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x10c898ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c899100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c899330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x10c899560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x10c899790 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x11d093030 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f64a0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x10dd65960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10dd65b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x11f64a310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f64a540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c8999c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f2157f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c899bf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c899e20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f215a20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f215c50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f64a770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10dd65dc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c89a050 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f215e80 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c89a280 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c89a4b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f64a9a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f2160b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c89a6e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f2162e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f216510 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c89a910 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c89ab40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f216840 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c89ad70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f64abd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c89afa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f64ae00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c89b4a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10dd65ff0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f64b030 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f64b260 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f64b490 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c89b1d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c89b6d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f216a70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f216ca0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f64b6c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f64b8f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f64bb20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f216ed0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f217100 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f64bd50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f64bf80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f217510 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f217740 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f64c1b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c89b900 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c89bb30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f64c3e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f217970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f217ba0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c89bd60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c89bf90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f217dd0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c89c1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10dd66220 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f64c610 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10dd66450 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c89c3f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f64c840 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f64ca70 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f64cca0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f64ced0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f218000 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f218230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f218520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c89c620 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c89c850 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f64d100 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f64d330 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10dd66680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f218750 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f64d560 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c89ca80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f218ba0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f64d790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10dd668b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c89ccb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f64d9c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10dd63190 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10dd66ae0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f64dbf0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c89cee0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c89d110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10dd66d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10dd66f40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f64de20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f64e050 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c89d340 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f64e280 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f64e4b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c89d570 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c89d7a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10dd67170 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c89d9d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f64e6e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f64e910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c89dc00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f64eb40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c89dfe0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f64ed70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10dd673a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f64efa0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10dd675d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f64f1d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f9aae90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x10c89e210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x11f64f400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x10dd67800 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x10be58860 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x10c89e440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x11f64f630 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x11f64f860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x11f64fa90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x11f64fcc0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x11f64fef0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x10dd67a30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x11f218f70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x11f650120 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x11f650350 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x10dd67c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x10dd67e90 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x10dd680c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x10dd682f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x10c89e670 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x11f2191a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x10dd68520 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x11f2193d0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x10dd68750 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f219600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x10dd68980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x11f219830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x10c89e8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x10dd68bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x10dd68de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c8a50a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f804080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x11f8042b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x11bfa7990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f219a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c8a52d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f650580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f8044e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x11f219c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x11f219ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c8a5500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f804710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x11f804940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f804b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f6507b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f804da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f21a0f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f804fd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f21a320 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f21a550 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f6509e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x11f805200 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x11f805430 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f650c10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x11f805660 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f650e40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f805890 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f21a780 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f805ac0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f651070 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x11f21a9b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x11f21abe0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f21ae10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x11f6512a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f6514d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f805cf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f651700 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f805f20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f651930 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x10c8a5730 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x11f651b60 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f21b040 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x11f21b270 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f21b4a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c8a5960 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f21b6d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f806150 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f21b900 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x11f651d90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x10c8a5b90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f651fc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x11f806380 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f806640 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c8a5dc0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f21bb30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f806870 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f21bd60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x11f21bf90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x11f21c1c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f806aa0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x11f806cd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c8a5ff0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c8a6220 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f806f00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c895980 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c895bb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x11f807130 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x10c895de0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c896010 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x11f807360 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x11f807590 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x11f898690 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x11f8988c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x11f21c3f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x11f898af0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x11f898d20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x10c896240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x11f898f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x11f21c620 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x11f21c850 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x11f21caf0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x11f21cd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f21cf50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c896470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f21d180 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f21d3b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f21d5e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c8966a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x11f21d810 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x10c8968d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x11f21da40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x10c896b00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x10c896d30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x11f899180 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x11f21dc70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x11f8993b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x11f8995e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x11f21dea0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x11f21e0d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x10c896f60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f21e300 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f6521f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f899810 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f899a40 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f899c70 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f899ea0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x11f652420 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x11f652650 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x11f652880 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x11f21e530 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x11f21e760 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x11f21e990 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x11f89a0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x11f89a300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f652ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f652ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f652f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f21ebc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f89a530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f89a760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f89a990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f653140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f653370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f6535a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11f89abc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11f89adf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11f6537d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11f89b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11f21edf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11f89b250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11f89b480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11f653a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11f653c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11f89b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x11f653e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x11f654090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x11f21f020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x11f654330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x11f21f250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x11f89b8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x11f21f480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x11f654560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x10c897190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x10c8973c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x11f89bb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x11f89bd40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x11f89bf70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x10c8976d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c897900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f836bd0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     1.00 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 2048 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified: layer  26: skipped\n",
      "llama_kv_cache_unified: layer  27: skipped\n",
      "llama_kv_cache_unified: layer  28: skipped\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: skipped\n",
      "llama_kv_cache_unified: layer  31: skipped\n",
      "llama_kv_cache_unified: layer  32: skipped\n",
      "llama_kv_cache_unified: layer  33: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_unified: size =   40.00 MiB (  2048 cells,   5 layers,  1 seqs), K (f16):   20.00 MiB, V (f16):   20.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 2048 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: skipped\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   232.00 MiB\n",
      "llama_kv_cache_unified: size =  232.00 MiB (  2048 cells,  29 layers,  1 seqs), K (f16):  116.00 MiB, V (f16):  116.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   522.00 MiB\n",
      "llama_context: graph nodes  = 1537\n",
      "llama_context: graph splits = 70 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'gemma3.vision.num_channels': '3', 'gemma3.vision.image_size': '896', 'gemma3.vision.attention.head_count': '16', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2', 'gemma3.mm.tokens_per_image': '256', 'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.chat_template': '{{ bos_token }} {%- if messages[0][\\'role\\'] == \\'system\\' -%} {%- if messages[0][\\'content\\'] is string -%} {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\\\n\\' -%} {%- else -%} {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\\\n\\' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message[\\'role\\'] == \\'assistant\\') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message[\\'role\\'] -%} {%- endif -%} {{ \\'<start_of_turn>\\' + role + \\'\\\\n\\' + (first_user_prefix if loop.first else \"\") }} {%- if message[\\'content\\'] is string -%} {{ message[\\'content\\'] | trim }} {%- elif message[\\'content\\'] is iterable -%} {%- for item in message[\\'content\\'] -%} {%- if item[\\'type\\'] == \\'image\\' -%} {{ \\'<start_of_image>\\' }} {%- elif item[\\'type\\'] == \\'text\\' -%} {{ item[\\'text\\'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ \\'<end_of_turn>\\\\n\\' }} {%- endfor -%} {%- if add_generation_prompt -%} {{\\'<start_of_turn>model\\\\n\\'}} {%- endif -%}', 'general.quantization_version': '2', 'gemma3.attention.head_count_kv': '4', 'tokenizer.ggml.padding_token_id': '0', 'gemma3.attention.sliding_window': '1024', 'gemma3.rope.freq_base': '1000000.000000', 'gemma3.rope.scaling.factor': '8.000000', 'tokenizer.ggml.model': 'llama', 'gemma3.context_length': '131072', 'gemma3.vision.feed_forward_length': '4304', 'gemma3.rope.scaling.type': 'linear', 'gemma3.vision.attention.layer_norm_epsilon': '0.000001', 'tokenizer.ggml.unknown_token_id': '3', 'gemma3.embedding_length': '2560', 'gemma3.vision.block_count': '27', 'gemma3.attention.value_length': '256', 'gemma3.vision.embedding_length': '1152', 'general.file_type': '2', 'gemma3.vision.patch_size': '14', 'gemma3.block_count': '34', 'gemma3.attention.head_count': '8', 'gemma3.attention.key_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma3.feed_forward_length': '10240', 'general.architecture': 'gemma3'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }} {%- if messages[0]['role'] == 'system' -%} {%- if messages[0]['content'] is string -%} {%- set first_user_prefix = messages[0]['content'] + '\\n' -%} {%- else -%} {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message['role'] == 'assistant') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message['role'] -%} {%- endif -%} {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }} {%- if message['content'] is string -%} {{ message['content'] | trim }} {%- elif message['content'] is iterable -%} {%- for item in message['content'] -%} {%- if item['type'] == 'image' -%} {{ '<start_of_image>' }} {%- elif item['type'] == 'text' -%} {{ item['text'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ '<end_of_turn>\\n' }} {%- endfor -%} {%- if add_generation_prompt -%} {{'<start_of_turn>model\\n'}} {%- endif -%}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n"
     ]
    }
   ],
   "source": [
    "# gated model login with Hugging Face CLI\n",
    "# Make sure you have the Hugging Face CLI installed and authenticated\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(os.getenv(\"HUGGING_FACE_API_KEY\"))\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download the model to 'models/gemma-3-4b-it-qat-q4_0.gguf'\n",
    "basic_deidentifier = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-3-4b-it-qat-q4_0-gguf\",\n",
    "    filename=\"gemma-3-4b-it-q4_0.gguf\",  \n",
    "    n_ctx=2048,\n",
    "    n_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab564dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7000d793c02041b6a48ff7fdfae826d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "              (act_fn): GELUActivation()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): GemmaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"google/gemma-2b\"\n",
    "LORA_MODEL_PATH = \"./gemma-deid-lora/checkpoint-60\"\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA weights\n",
    "gemma_finetuned_model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "gemma_finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0634333",
   "metadata": {},
   "outputs": [],
   "source": [
    "deidentifier_prompt = \"\"\"You are a local language model responsible for enforcing HIPAA compliance by identifying and \n",
    "        removing all Protected Health Information (PHI) from clinical text and structured data before \n",
    "        it is passed to an external system. Your task is to remove all 18 identifiers defined under \n",
    "        HIPAA's Safe Harbor rule while preserving the clinical meaning of the data.\n",
    "\n",
    "        Redact all identifiers like names, dates, addresses, SSNs, etc. with placeholder [REDACTED], without summarizing or altering clinical facts.\"\"\"\n",
    "\n",
    "\n",
    "def _build_hipaa_prompt(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the common HIPAA compliance prompt for de-identification.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt for de-identification\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "        {deidentifier_prompt}\n",
    "\n",
    "        ---\n",
    "        <data_with_phi>\n",
    "        {user_data}\n",
    "        </data_with_phi>\n",
    "        <data_hipaa_compliant>\n",
    "    \"\"\"\n",
    "\n",
    "def _extract_redacted_content(raw_output: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the redacted content from model output.\n",
    "    \n",
    "    Args:\n",
    "        raw_output: Raw output from the model\n",
    "        prompt: Original prompt used\n",
    "    \n",
    "    Returns:\n",
    "        Extracted redacted content\n",
    "    \"\"\"\n",
    "    if \"<data_hipaa_compliant>\" in raw_output:\n",
    "        redacted_part = raw_output.split(\"<data_hipaa_compliant>\")[-1]\n",
    "        redacted_part = redacted_part.split(\"</data_hipaa_compliant>\")[0].strip()\n",
    "    else:\n",
    "        # Fallback: remove the prompt from the beginning\n",
    "        redacted_part = raw_output[len(prompt):].strip()\n",
    "    \n",
    "    return redacted_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0788c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentify_with_basic_gemma(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    De-identify clinical text using the basic Gemma model via llama-cpp-python.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        De-identified text with PHI redacted\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = basic_deidentifier.create_chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": deidentifier_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_data}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        raw_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"ðŸ“„ Original note: {user_data}\")\n",
    "        print(\"ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\")\n",
    "        print(f\"ðŸ“„ Redacted note: {raw_output}\")\n",
    "\n",
    "        return raw_output\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during basic de-identification: {e}\")\n",
    "        return f\"[DEIDENTIFICATION_ERROR: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba7a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentify_with_finetuned_gemma(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    De-identify clinical text using the fine-tuned Gemma model with LoRA weights.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        De-identified text with PHI redacted\n",
    "    \"\"\"\n",
    "    prompt = _build_hipaa_prompt(user_data)\n",
    "    \n",
    "    try:\n",
    "        device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = gemma_finetuned_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        raw_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        redacted_content = _extract_redacted_content(raw_output, prompt)\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"ðŸ“„ Original note: {user_data}\")\n",
    "        print(\"ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\")\n",
    "        print(f\"ðŸ“„ Redacted note: {redacted_content}\")\n",
    "        \n",
    "        return redacted_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during fine-tuned de-identification: {e}\")\n",
    "        return f\"[DEIDENTIFICATION_ERROR: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b282f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained Clinical Assistant with all configurations built-in\n",
    "class ClinicalAssistant:\n",
    "    \"\"\"Self-contained clinical assistant with built-in configurations and tools\"\"\"\n",
    "    \n",
    "    def __init__(self, deidentify_model: str = \"finetuned\"):\n",
    "        \"\"\"\n",
    "        Initialize the Clinical Assistant with built-in configurations\n",
    "        \n",
    "        Args:\n",
    "            deidentify_model: \"basic\" or \"finetuned\" for de-identification model\n",
    "        \"\"\"\n",
    "        self.deidentify_model = deidentify_model\n",
    "        \n",
    "        # Initialize OpenAI configuration\n",
    "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        self.model = \"gpt-4.1\"\n",
    "        self.max_completion_tokens = 5000\n",
    "        self.temperature = 0.1\n",
    "        \n",
    "        # Tool definitions\n",
    "        self.tool_definitions = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_observations\",\n",
    "                    \"description\": \"Retrieve laboratory test results and clinical observations for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_conditions\",\n",
    "                    \"description\": \"Retrieve patient diagnosis information, medical conditions, and medical history\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_medications\",\n",
    "                    \"description\": \"Retrieve current and past medications for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_careplans\",\n",
    "                    \"description\": \"Retrieve care plans and treatment plans for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_procedures\",\n",
    "                    \"description\": \"Retrieve medical procedures and interventions performed on a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_imaging_studies\",\n",
    "                    \"description\": \"Retrieve imaging studies and radiology reports for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_immunizations\",\n",
    "                    \"description\": \"Retrieve vaccination history and immunization records for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_allergies\",\n",
    "                    \"description\": \"Retrieve allergy information and adverse reactions for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Function mapping - directly use generate_note_auto with appropriate table names\n",
    "        self.function_map = {\n",
    "            \"get_patient_observations\": lambda patient_id: generate_note_auto(\"observations\", patient_id),\n",
    "            \"get_patient_conditions\": lambda patient_id: generate_note_auto(\"conditions\", patient_id),\n",
    "            \"get_patient_medications\": lambda patient_id: generate_note_auto(\"medications\", patient_id),\n",
    "            \"get_patient_careplans\": lambda patient_id: generate_note_auto(\"careplans\", patient_id),\n",
    "            \"get_patient_procedures\": lambda patient_id: generate_note_auto(\"procedures\", patient_id),\n",
    "            \"get_patient_imaging_studies\": lambda patient_id: generate_note_auto(\"imaging_studies\", patient_id),\n",
    "            \"get_patient_immunizations\": lambda patient_id: generate_note_auto(\"immunizations\", patient_id),\n",
    "            \"get_patient_allergies\": lambda patient_id: generate_note_auto(\"allergies\", patient_id)\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Clinical Assistant initialized with {self.deidentify_model} de-identification model!\")\n",
    "    \n",
    "    def _build_system_prompt(self) -> str:\n",
    "        \"\"\"Build the system prompt for the clinical assistant\"\"\"\n",
    "        return f\"\"\"You are an AI Clinical Reasoning Assistant with expertise in internal medicine. \n",
    "        Analyze clinical data and respond to patient-specific questions using structured reasoning.\n",
    "\n",
    "        Available Tools:\n",
    "        - get_patient_observations(): Laboratory test results and clinical observations\n",
    "        - get_patient_conditions(): Diagnosis history, medical conditions, and medical history\n",
    "        - get_patient_medications(): Current and past medications\n",
    "        - get_patient_careplans(): Care plans and treatment plans\n",
    "        - get_patient_procedures(): Medical procedures and interventions\n",
    "        - get_patient_imaging_studies(): Imaging studies and radiology reports\n",
    "        - get_patient_immunizations(): Vaccination history and immunization records\n",
    "        - get_patient_allergies(): Allergy information and adverse reactions\n",
    "\n",
    "        Clinical Reasoning Framework:\n",
    "        1. Analyze the question to determine needed information\n",
    "        2. Use appropriate tools to gather relevant patient data\n",
    "        3. Synthesize findings from multiple data sources\n",
    "        4. Provide clear, evidence-based responses with clinical reasoning\n",
    "\n",
    "        Rules:\n",
    "        - Use ONLY data provided by tool outputs\n",
    "        - Reference relative timeframes when provided (e.g., \"Day 0\", \"Post-op Day 3\")\n",
    "        - Acknowledge limitations if data is insufficient\n",
    "        - Suggest additional information needed when applicable\n",
    "        - Consider interactions between medications, conditions, and procedures\n",
    "        - Always check for allergies before recommending treatments\n",
    "\n",
    "        Current date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _de_identify_data(self, data: str) -> str:\n",
    "        \"\"\"De-identify text data using the specified deidentify model\"\"\"\n",
    "        if self.deidentify_model == \"basic\":\n",
    "            return deidentify_with_basic_gemma(data)\n",
    "        elif self.deidentify_model == \"finetuned\":\n",
    "            return deidentify_with_finetuned_gemma(data)        \n",
    "    \n",
    "    def _execute_tool_call(self, function_name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute a tool function call with de-identification\"\"\"\n",
    "        if function_name not in self.function_map:\n",
    "            return f\"Error: Unknown function: {function_name}\"\n",
    "        \n",
    "        print(f\"Executing tool call: {function_name}\")\n",
    "        try:\n",
    "            func = self.function_map[function_name]\n",
    "            raw_result = func(**arguments)\n",
    "            return self._de_identify_data(raw_result)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {function_name}: {str(e)}\"\n",
    "\n",
    "    def process_query(self, query: str, patient_id: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Process a clinical query using OpenAI GPT with tool calling\n",
    "        \n",
    "        Args:\n",
    "            query: The clinical question to answer\n",
    "            patient_id: Optional patient ID if known\n",
    "            \n",
    "        Returns:\n",
    "            Clinical assistant response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build messages\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self._build_system_prompt()},\n",
    "                {\"role\": \"user\", \"content\": f\"Clinical query: {query}\"}\n",
    "            ]\n",
    "            \n",
    "            if patient_id:\n",
    "                messages[-1][\"content\"] += f\"\\nPatient ID: {patient_id}\"\n",
    "            \n",
    "            # Get initial response from LLM\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                tools=self.tool_definitions,\n",
    "                tool_choice=\"auto\",\n",
    "                max_completion_tokens=self.max_completion_tokens,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            \n",
    "            # Handle tool calls if any\n",
    "            if response.choices[0].message.tool_calls:\n",
    "                current_messages = messages.copy()\n",
    "                current_messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response.choices[0].message.content,\n",
    "                    \"tool_calls\": response.choices[0].message.tool_calls\n",
    "                })\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in response.choices[0].message.tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments)\n",
    "                    \n",
    "                    # Execute tool call with de-identification\n",
    "                    tool_result = self._execute_tool_call(function_name, function_args)\n",
    "                    \n",
    "                    current_messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": tool_result\n",
    "                    })\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                final_response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=current_messages,\n",
    "                    tools=self.tool_definitions,\n",
    "                    tool_choice=\"auto\",\n",
    "                    max_completion_tokens=self.max_completion_tokens,\n",
    "                    temperature=self.temperature\n",
    "                )\n",
    "                return final_response.choices[0].message.content\n",
    "            else:\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"âŒ Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefdcd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Clinical Assistant with Integrated De-identification\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Clinical Assistant with Different De-identification Models\n",
    "print(\"ðŸ§ª Testing Clinical Assistant with Integrated De-identification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample clinical queries to test the system with all available tools\n",
    "sample_queries = [\n",
    "    \"Can you summarize this patient's medical conditions and current medications?\",\n",
    "    \"Does this patient have any allergies I should be aware of before prescribing?\",\n",
    "    \"Is the patient up to date on their vaccinations?\",\n",
    "    \"The patient wants to try and get pregnant, is there any relevant medical history or medications that should be considered?\",\n",
    "]\n",
    "\n",
    "# Enhanced interactive testing function that creates assistant instances\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def test_query(query_text: str, patient_id: str = \"12345678\", deidentify_model: str = \"finetuned\"):\n",
    "    \"\"\"\n",
    "    Test function that creates a Clinical Assistant instance with the specified model\n",
    "    \n",
    "    Args:\n",
    "        query_text: The clinical question to ask\n",
    "        patient_id: Patient identifier (default: \"12345678\")\n",
    "        deidentify_model: \"finetuned\" or \"basic\" for de-identification model\n",
    "    \n",
    "    Returns:\n",
    "        Assistant response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create assistant instance with the specified de-identification model\n",
    "        assistant = ClinicalAssistant(deidentify_model=deidentify_model)\n",
    "        \n",
    "        model_name = \"Fine-tuned Gemma\" if deidentify_model == \"finetuned\" else \"Basic Gemma\"\n",
    "        header_md = f\"### ðŸ©º Clinical Query: {query_text}\\n\\n**Patient ID:** {patient_id}\\n**De-identification Model:** {model_name}\"\n",
    "        \n",
    "        display(Markdown(header_md))\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### ðŸ¤– Assistant Response\"))\n",
    "\n",
    "        response = assistant.process_query(query_text, patient_id)\n",
    "        display(Markdown(f\"```\\n{response}\\n```\"))\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ Error: {str(e)}\"\n",
    "        display(Markdown(error_msg))\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a13644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing Fine-tuned Gemma model:\n",
      "âœ… Clinical Assistant initialized with finetuned de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: Does this patient have any allergies I should be aware of before prescribing?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Fine-tuned Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_allergies\n",
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, SSN: 999-26-7676, born on 1999â€‘06â€‘07 in Oakes, North Dakota, presented to TOWNER COUNTY MEDICAL CENTER INC (HWYâ€¯281N, CANDO, NDâ€¯58324) on 2000â€‘11â€‘20 for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. Shiloh Larson, general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, NDâ€¯58324 (Towner County, FIPSâ€¯38095), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: [REDACTED], SSN: [REDACTED], born on [REDACTED] in [REDACTED], presented to [REDACTED] on [REDACTED] for an encounter for problem [REDACTED] related to [REDACTED] with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of [REDACTED].  \n",
      "The visit was classified as [REDACTED], with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was [REDACTED], leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at [REDACTED], and is a white, nonâ€‘Hispanic female with no recorded marital status.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "There is no documented allergy information available for this patient in the current records. Therefore, I cannot confirm whether the patient has any allergies you should be aware of before prescribing.\n",
       "\n",
       "Recommendation:\n",
       "- If possible, obtain a direct allergy history from the patient or review additional records to ensure safe prescribing.\n",
       "- If you have specific concerns about certain medications or classes, please specify, and I can assist further.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ” Testing Basic Gemma model:\n",
      "âœ… Clinical Assistant initialized with basic de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: Does this patient have any allergies I should be aware of before prescribing?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Basic Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_allergies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   22604.91 ms\n",
      "llama_perf_context_print: prompt eval time =   22602.73 ms /   755 tokens (   29.94 ms per token,    33.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   68461.57 ms /   914 runs   (   74.90 ms per token,    13.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   92091.28 ms /  1669 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, SSN: 999-26-7676, born on 1999â€‘06â€‘07 in Oakes, North Dakota, presented to TOWNER COUNTY MEDICAL CENTER INC (HWYâ€¯281N, CANDO, NDâ€¯58324) on 2000â€‘11â€‘20 for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. Shiloh Larson, general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, NDâ€¯58324 (Towner County, FIPSâ€¯38095), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: otal claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, NDâ€¯58324 (Towner County, FIPSâ€¯38095), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "\"\"\"\n",
      "\n",
      "data_hipaa_compliant = redact_phi(data_with_phi)\n",
      "print(data_hipaa_compliant)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "Patient: Ms. [REDACTED], SSN: [REDACTED], born on [REDACTED] in [REDACTED], presented to [REDACTED] ([REDACTED]) on [REDACTED] for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. [REDACTED], general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. [REDACTED] resides at [REDACTED] ([REDACTED]), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "This patient has a documented lifelong allergy to animal dander, which causes moderate rhinoconjunctivitis and mild skin eruptions. There are no other allergies listed in the available data.\n",
       "\n",
       "Clinical recommendation:\n",
       "- Be aware of this allergy when prescribing, especially if considering medications or treatments that may contain animal-derived components (e.g., certain vaccines, antitoxins, or biologics).\n",
       "- No drug or medication allergies are documented in the current record.\n",
       "\n",
       "If you are considering a specific medication or treatment, please specify so I can check for any relevant interactions or additional precautions.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'This patient has a documented lifelong allergy to animal dander, which causes moderate rhinoconjunctivitis and mild skin eruptions. There are no other allergies listed in the available data.\\n\\nClinical recommendation:\\n- Be aware of this allergy when prescribing, especially if considering medications or treatments that may contain animal-derived components (e.g., certain vaccines, antitoxins, or biologics).\\n- No drug or medication allergies are documented in the current record.\\n\\nIf you are considering a specific medication or treatment, please specify so I can check for any relevant interactions or additional precautions.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test with both de-identification models separately\n",
    "test_patient_id = \"1329b83e-ea69-d184-b4af-0d2a8e07896e\"\n",
    "query = sample_queries[1]\n",
    "\n",
    "print(\"ðŸ” Testing Fine-tuned Gemma model:\")\n",
    "test_query(query, test_patient_id, \"finetuned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ” Testing Basic Gemma model:\")\n",
    "test_query(query, test_patient_id, \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eae170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing Fine-tuned Gemma model:\n",
      "âœ… Clinical Assistant initialized with finetuned de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: Is the patient up to date on their vaccinations?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Fine-tuned Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_immunizations\n",
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, born on 06/07/1999 (SSN: 999-26-7676), presented to TOWNER COUNTY MEDICAL CENTER on 08/30/2022 for a general wellness examination. Dr. Sal Lehner documented a routine physical, noting she received the Influenza seasonal injectable preservative free vaccine (codeâ€¯140) at ageâ€¯23, and discussed her healthcare expenses of $127,546.31 against coverage of $673,780.87, with a current income of $63,061; she resides at 523â€¯O'Kon Orchard, Cando, North Dakota, 58324. Followâ€‘up is scheduled as needed; the total claim cost was $593.90, with $475.12 covered by her payer.\n",
      "ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: [REDACTED], born on [REDACTED] (SSN: [REDACTED]), presented to [REDACTED] on [REDACTED] for a general wellness examination. Dr. [REDACTED] documented a routine physical, noting she received the Influenza seasonal injectable preservative free vaccine (codeâ€¯140) at ageâ€¯[REDACTED], and discussed her healthcare expenses of $127,546.31 against coverage of $673,780.87, with a current income of $63,061; she resides at [REDACTED], North Dakota, 58324. Followâ€‘up is scheduled as needed; the total claim cost was $593.90, with $475.12 covered by her payer.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "The available immunization record only documents that the patient received a seasonal influenza vaccine at a previous visit. There is no information provided about other routine vaccinations (such as Tdap, MMR, varicella, pneumococcal, COVID-19, or others recommended by age and risk factors).\n",
       "\n",
       "Based on the data provided, I cannot confirm whether the patient is fully up to date on all recommended vaccinations. \n",
       "\n",
       "Recommendation:\n",
       "- Additional immunization records or a detailed vaccination history are needed to accurately determine if the patient is up to date.\n",
       "- Consider reviewing the patient's full immunization record or contacting their primary care provider for a comprehensive assessment.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ” Testing Basic Gemma model:\n",
      "âœ… Clinical Assistant initialized with basic de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: Is the patient up to date on their vaccinations?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Basic Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_immunizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 121 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   22604.91 ms\n",
      "llama_perf_context_print: prompt eval time =    4309.29 ms /   205 tokens (   21.02 ms per token,    47.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10369.49 ms /   153 runs   (   67.77 ms per token,    14.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   14769.54 ms /   358 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, born on 06/07/1999 (SSN: 999-26-7676), presented to TOWNER COUNTY MEDICAL CENTER on 08/30/2022 for a general wellness examination. Dr. Sal Lehner documented a routine physical, noting she received the Influenza seasonal injectable preservative free vaccine (codeâ€¯140) at ageâ€¯23, and discussed her healthcare expenses of $127,546.31 against coverage of $673,780.87, with a current income of $63,061; she resides at 523â€¯O'Kon Orchard, Cando, North Dakota, 58324. Followâ€‘up is scheduled as needed; the total claim cost was $593.90, with $475.12 covered by her payer.\n",
      "ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: Ms. [REDACTED], born on [REDACTED], presented to TOWNER COUNTY MEDICAL CENTER on [REDACTED] for a general wellness examination. Dr. [REDACTED] documented a routine physical, noting she received the Influenza seasonal injectable preservative free vaccine (codeâ€¯[REDACTED]) at ageâ€¯[REDACTED], and discussed her healthcare expenses of $[REDACTED] against coverage of $[REDACTED], with a current income of $[REDACTED]; she resides at [REDACTED], [REDACTED], [REDACTED]. Followâ€‘up is scheduled as needed; the total claim cost was $[REDACTED], with $[REDACTED] covered by her payer.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "The available immunization record only documents that the patient received a seasonal influenza vaccine at a previous visit. There is no information provided about other routine adult vaccinations (such as Tdap, pneumococcal, shingles, COVID-19, or others).\n",
       "\n",
       "Based on the data available, I cannot confirm whether the patient is fully up to date on all recommended vaccinations. Additional immunization records or a more comprehensive vaccination history would be needed to answer this question definitively.\n",
       "\n",
       "Recommendation:\n",
       "- Review the patient's full immunization record or obtain further details to assess if all age-appropriate vaccines are current.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"The available immunization record only documents that the patient received a seasonal influenza vaccine at a previous visit. There is no information provided about other routine adult vaccinations (such as Tdap, pneumococcal, shingles, COVID-19, or others).\\n\\nBased on the data available, I cannot confirm whether the patient is fully up to date on all recommended vaccinations. Additional immunization records or a more comprehensive vaccination history would be needed to answer this question definitively.\\n\\nRecommendation:\\n- Review the patient's full immunization record or obtain further details to assess if all age-appropriate vaccines are current.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Test with both de-identification models separately\n",
    "test_patient_id = \"1329b83e-ea69-d184-b4af-0d2a8e07896e\"\n",
    "query = sample_queries[2]\n",
    "\n",
    "print(\"ðŸ” Testing Fine-tuned Gemma model:\")\n",
    "test_query(query, test_patient_id, \"finetuned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ” Testing Basic Gemma model:\")\n",
    "test_query(query, test_patient_id, \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c57d5c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing Fine-tuned Gemma model:\n",
      "âœ… Clinical Assistant initialized with finetuned de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: The patient wants to try and get pregnant, is there any relevant medical history or medications that should be considered?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Fine-tuned Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_conditions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, born on 06/07/1999 in Oakes, North Dakota (SSN: 999â€‘26â€‘7676), white, nonâ€‘Hispanic, female, presented to TOWNER COUNTY MEDICAL CENTER on 12/14/2015 for a medication review due (situation) at age 16; she resides at 523â€¯O'Kon Orchard, Cando, North Dakotaâ€¯58324 (Towner County, FIPSâ€¯38095, latitudeâ€¯48.4746068, longitudeâ€¯â€‘99.2336557) and holds driverâ€™s license S99963354 and passport X23683209X. Dr. Jarvis Ankunding, a general practitioner, recorded the encounter as an outpatient checkâ€‘up (procedure) with a base cost of $96.45, a total claim of $1,257.75 fully covered by the payer, while her cumulative healthcare expenses amount to $127,546.31 against coverage of $673,780.87. The encounter was classified under SNOMEDâ€‘CT code 314529007 (Medication review due) and was documented as an outpatient encounter with a provider specialty of general practice. Followâ€‘up was scheduled for a future date to review medication adherence and any new concerns.\n",
      "ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: [REDACTED], born on [REDACTED] in [REDACTED], white, nonâ€‘Hispanic, female, presented to [REDACTED] on [REDACTED] for a medication review due at age [REDACTED]; she resides at [REDACTED] and holds driverâ€™s license [REDACTED] and passport [REDACTED]. Dr. [REDACTED], a general practitioner, recorded the encounter as an outpatient checkâ€‘up with a base cost of $96.45, a total claim of $1,257.75 fully covered by the payer, while her cumulative healthcare expenses amount to $127,546.31 against coverage of $673,780.87. The encounter was classified under SNOMEDâ€‘CT code 314529007 (Medication review due) and was documented as an outpatient encounter with a provider specialty of general practice. Followâ€‘up was scheduled for a future date to review medication adherence and any new concerns.\n",
      "Executing tool call: get_patient_medications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Ms. Yaeko Ming Kshlerin, Ms. born 06/07/1999 (age 23 at the start of her medication), a white, nonâ€‘Hispanic female from Oakes, North Dakota, presented for an outpatient checkâ€‘up at TOWNER COUNTY MEDICAL CENTER (HWYâ€¯281â€¯N, Cando, NDâ€¯58324) under the care of Dr. Jarvis Ankunding, general practice. She received Acetaminophen 300â€¯mg / Hydrocodone Bitartrate 5â€¯mg oral tablets (pharmacy codeâ€¯856987) from 05/09/2023â€¯04:52:06 to 09/05/2023â€¯04:52:06, with three dispenses totalling $97.29 (base cost $32.43, fully covered by payer), and the encounter itself cost $96.45 (claim cost $1,053.14, fully covered). Her home address is 523â€¯O'Konâ€¯Orchard, Cando, NDâ€¯58324 (latâ€¯48.4746, lonâ€¯-99.2337), she resides in Towner County (FIPSâ€¯38095), and her SSN is 999â€‘26â€‘7676, driverâ€™s license S99963354, passport X23683209X.  Overall, her healthcare expenses total $127,546.31 against coverage of $673,780.87, with an annual income of $63,061.\n",
      "ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Ms. [REDACTED] was born [REDACTED] (age 23 at the start of her medication), a white, nonâ€‘Hispanic female from [REDACTED], North Dakota, presented for an outpatient checkâ€‘up at [REDACTED] under the care of Dr. [REDACTED], general practice. She received Acetaminophen 300â€¯mg / Hydrocodone Bitartrate 5â€¯mg oral tablets (pharmacy codeâ€¯856987) from [REDACTED] to [REDACTED], with three dispenses totalling $97.29 (base cost $32.43, fully covered by payer), and the encounter itself cost $96.45 (claim cost $1,053.14, fully covered). Her home address is [REDACTED], she resides in [REDACTED], and her SSN is [REDACTED], driverâ€™s license S[REDACTED], passport X[REDACTED].  Overall, her healthcare expenses total $127,546.31 against coverage of $673,780.87, with an annual income of $63,061.\n",
      "Executing tool call: get_patient_allergies\n",
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, SSN: 999-26-7676, born on 1999â€‘06â€‘07 in Oakes, North Dakota, presented to TOWNER COUNTY MEDICAL CENTER INC (HWYâ€¯281N, CANDO, NDâ€¯58324) on 2000â€‘11â€‘20 for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. Shiloh Larson, general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, NDâ€¯58324 (Towner County, FIPSâ€¯38095), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "ðŸ”’ Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: [REDACTED], SSN: [REDACTED], born on [REDACTED] in [REDACTED], presented to [REDACTED] on [REDACTED] for an encounter for problem [REDACTED] related to [REDACTED] with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of [REDACTED].  \n",
      "The visit was classified as [REDACTED], with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was [REDACTED], leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at [REDACTED], and is a white, nonâ€‘Hispanic female with no recorded marital status.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Summary of findings:\n",
       "- Medical conditions: The available data does not specify any chronic medical conditions or diagnoses relevant to pregnancy (such as diabetes, hypertension, thyroid disease, or reproductive disorders).\n",
       "- Medications: The patient has previously received Acetaminophen 300 mg / Hydrocodone Bitartrate 5 mg oral tablets. This is a combination opioid analgesic, which is not recommended during pregnancy due to potential risks to the fetus (including neonatal opioid withdrawal syndrome and possible teratogenicity).\n",
       "- Allergies: The patient has a history of moderate rhinoconjunctivitis and mild skin eruptions, suggesting possible allergic tendencies, but no specific drug allergies are documented.\n",
       "\n",
       "Clinical reasoning:\n",
       "- If the patient is still taking Acetaminophen/Hydrocodone, this medication should be reviewed and ideally discontinued or substituted with a safer alternative prior to conception.\n",
       "- Allergic tendencies may be relevant if considering medications or supplements during pregnancy.\n",
       "- No other significant medical history is documented that would directly impact pregnancy planning.\n",
       "\n",
       "Recommendations:\n",
       "1. Review current medication useâ€”if the patient is still on opioid analgesics, discuss tapering and alternative pain management strategies.\n",
       "2. Consider preconception counseling, including review of immunization status, folic acid supplementation, and screening for other medical conditions not listed here.\n",
       "3. Monitor for allergic reactions if new medications or prenatal vitamins are started.\n",
       "\n",
       "Limitations:\n",
       "- The data does not provide a comprehensive list of all medical conditions or a current medication list. Additional information about current health status, menstrual/reproductive history, and other medications would be helpful.\n",
       "\n",
       "If you need a more detailed assessment, please provide or request additional information regarding current medications, medical conditions, or recent laboratory results.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ” Testing Basic Gemma model:\n",
      "âœ… Clinical Assistant initialized with basic de-identification model!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ©º Clinical Query: The patient wants to try and get pregnant, is there any relevant medical history or medications that should be considered?\n",
       "\n",
       "**Patient ID:** 1329b83e-ea69-d184-b4af-0d2a8e07896e\n",
       "**De-identification Model:** Basic Gemma"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ðŸ¤– Assistant Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing tool call: get_patient_conditions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   22604.91 ms\n",
      "llama_perf_context_print: prompt eval time =    6942.13 ms /   450 tokens (   15.43 ms per token,    64.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16781.24 ms /   231 runs   (   72.65 ms per token,    13.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   23865.82 ms /   681 tokens\n",
      "Llama.generate: 111 prefix-match hit, remaining 413 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, born on 06/07/1999 in Oakes, North Dakota (SSN: 999â€‘26â€‘7676), white, nonâ€‘Hispanic, female, presented to TOWNER COUNTY MEDICAL CENTER on 12/14/2015 for a medication review due (situation) at age 16; she resides at 523â€¯O'Kon Orchard, Cando, North Dakotaâ€¯58324 (Towner County, FIPSâ€¯38095, latitudeâ€¯48.4746068, longitudeâ€¯â€‘99.2336557) and holds driverâ€™s license S99963354 and passport X23683209X. Dr. Jarvis Ankunding, a general practitioner, recorded the encounter as an outpatient checkâ€‘up (procedure) with a base cost of $96.45, a total claim of $1,257.75 fully covered by the payer, while her cumulative healthcare expenses amount to $127,546.31 against coverage of $673,780.87. The encounter was classified under SNOMEDâ€‘CT code 314529007 (Medication review due) and was documented as an outpatient encounter with a provider specialty of general practice. Followâ€‘up was scheduled for a future date to review medication adherence and any new concerns.\n",
      "ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: Ms. [REDACTED], born on [REDACTED] in [REDACTED], [REDACTED], white, nonâ€‘Hispanic, female, presented to [REDACTED] on [REDACTED] for a medication review due (situation) at age [REDACTED]; she resides at [REDACTED], [REDACTED] ([REDACTED], FIPS [REDACTED], latitude [REDACTED], longitude [REDACTED]) and holds driverâ€™s license [REDACTED] and passport [REDACTED]. Dr. [REDACTED] recorded the encounter as an outpatient checkâ€‘up (procedure) with a base cost of $[REDACTED], a total claim of $[REDACTED] fully covered by the payer, while her cumulative healthcare expenses amount to $[REDACTED] against coverage of $[REDACTED]. The encounter was classified under SNOMEDâ€‘CT code [REDACTED] and was documented as an outpatient encounter with a provider specialty of general practice. Followâ€‘up was scheduled for a future date to review medication adherence and any new concerns.\n",
      "Executing tool call: get_patient_medications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   22604.91 ms\n",
      "llama_perf_context_print: prompt eval time =    4051.69 ms /   413 tokens (    9.81 ms per token,   101.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23200.34 ms /   330 runs   (   70.30 ms per token,    14.22 tokens per second)\n",
      "llama_perf_context_print:       total time =   27459.69 ms /   743 tokens\n",
      "Llama.generate: 111 prefix-match hit, remaining 307 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Ms. Yaeko Ming Kshlerin, Ms. born 06/07/1999 (age 23 at the start of her medication), a white, nonâ€‘Hispanic female from Oakes, North Dakota, presented for an outpatient checkâ€‘up at TOWNER COUNTY MEDICAL CENTER (HWYâ€¯281â€¯N, Cando, NDâ€¯58324) under the care of Dr. Jarvis Ankunding, general practice. She received Acetaminophen 300â€¯mg / Hydrocodone Bitartrate 5â€¯mg oral tablets (pharmacy codeâ€¯856987) from 05/09/2023â€¯04:52:06 to 09/05/2023â€¯04:52:06, with three dispenses totalling $97.29 (base cost $32.43, fully covered by payer), and the encounter itself cost $96.45 (claim cost $1,053.14, fully covered). Her home address is 523â€¯O'Konâ€¯Orchard, Cando, NDâ€¯58324 (latâ€¯48.4746, lonâ€¯-99.2337), she resides in Towner County (FIPSâ€¯38095), and her SSN is 999â€‘26â€‘7676, driverâ€™s license S99963354, passport X23683209X.  Overall, her healthcare expenses total $127,546.31 against coverage of $673,780.87, with an annual income of $63,061.\n",
      "ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Ms. [REDACTED] [REDACTED] [REDACTED], Ms. born 06/07/1999 (age 23 at the start of her medication), a white, nonâ€‘Hispanic female from [REDACTED], North Dakota, presented for an outpatient checkâ€‘up at [REDACTED] ([REDACTED], [REDACTED], [REDACTED]) under the care of Dr. [REDACTED] [REDACTED], general practice. She received Acetaminophen 300â€¯mg / Hydrocodone Bitartrate 5â€¯mg oral tablets (pharmacy codeâ€¯856987) from 05/09/2023â€¯04:52:06 to 09/05/2023â€¯04:52:06, with three dispenses totalling $[REDACTED] ([REDACTED], fully covered by payer), and the encounter itself cost $[REDACTED] ([REDACTED], fully covered). Her home address is [REDACTED], [REDACTED], [REDACTED] (lat [REDACTED], lon [REDACTED]), she resides in [REDACTED] ([REDACTED]), and her SSN is [REDACTED], driverâ€™s license [REDACTED], passport [REDACTED]. Overall, her healthcare expenses total $[REDACTED] against coverage of $[REDACTED], with an annual income of $[REDACTED].\n",
      "Executing tool call: get_patient_allergies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   22604.91 ms\n",
      "llama_perf_context_print: prompt eval time =    3321.41 ms /   307 tokens (   10.82 ms per token,    92.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16653.61 ms /   209 runs   (   79.68 ms per token,    12.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   20102.28 ms /   516 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Original note: Patient: Ms. Yaeko Ming Kshlerin, SSN: 999-26-7676, born on 1999â€‘06â€‘07 in Oakes, North Dakota, presented to TOWNER COUNTY MEDICAL CENTER INC (HWYâ€¯281N, CANDO, NDâ€¯58324) on 2000â€‘11â€‘20 for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. Shiloh Larson, general practice.  \n",
      "The visit was classified as ambulatory, with a base encounter cost of $96.45 and a total claim cost of $483.55; payer coverage was $0.00, leaving her responsible for the full cost, while her total healthcare expenses amount to $127,546.31 against a coverage pool of $673,780.87, and her annual income is $63,061.  \n",
      "Ms. Kshlerin resides at 523 O'Kon Orchard, Cando, NDâ€¯58324 (Towner County, FIPSâ€¯38095), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n",
      "ðŸ”’ Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\n",
      "ðŸ“„ Redacted note: Patient: [REDACTED], SSN: [REDACTED], born on [REDACTED] in [REDACTED], presented to [REDACTED] ([REDACTED]) on [REDACTED] for an encounter for problem (procedure) related to allergic disposition; she reports a lifelong allergy to animal dander with moderate rhinoconjunctivitis and mild skin eruptions, and she is currently under the care of Dr. [REDACTED], general practice.\n",
      "The visit was classified as ambulatory, with a base encounter cost of [REDACTED] and a total claim cost of [REDACTED]; payer coverage was [REDACTED], leaving her responsible for the full cost, while her total healthcare expenses amount to [REDACTED] against a coverage pool of [REDACTED], and her annual income is [REDACTED].\n",
      "[REDACTED] resides at [REDACTED] ([REDACTED]), and is a white, nonâ€‘Hispanic female with no recorded marital status.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Summary of findings:\n",
       "- Medical history: No specific chronic medical conditions or diagnoses relevant to pregnancy were identified in the available data.\n",
       "- Medications: The patient was prescribed Acetaminophen 300 mg / Hydrocodone Bitartrate 5 mg oral tablets from May to September 2023. There is no evidence of ongoing use or other current medications.\n",
       "- Allergies: The patient has a lifelong allergy to animal dander, causing moderate rhinoconjunctivitis and mild skin eruptions. No medication allergies are documented.\n",
       "\n",
       "Clinical considerations:\n",
       "- Hydrocodone/acetaminophen is not recommended during pregnancy due to potential risks of opioid exposure to the fetus. However, the prescription ended in September 2023, so unless the patient is still taking this medication, it should not impact current pregnancy planning.\n",
       "- No other medications or medical conditions were identified that would directly impact fertility or pregnancy safety.\n",
       "- No medication allergies are documented, but animal dander allergy is noted (not relevant to pregnancy).\n",
       "\n",
       "Recommendations:\n",
       "- Confirm that the patient is not currently taking hydrocodone/acetaminophen or any other potentially teratogenic medications.\n",
       "- If she is taking any other medications or has additional medical history not captured here, further review is warranted.\n",
       "- Routine preconception counseling is advised, including folic acid supplementation, review of immunization status, and assessment for any chronic conditions not listed here.\n",
       "\n",
       "If more detailed or recent medication or condition data is needed, please specify.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Summary of findings:\\n- Medical history: No specific chronic medical conditions or diagnoses relevant to pregnancy were identified in the available data.\\n- Medications: The patient was prescribed Acetaminophen 300 mg / Hydrocodone Bitartrate 5 mg oral tablets from May to September 2023. There is no evidence of ongoing use or other current medications.\\n- Allergies: The patient has a lifelong allergy to animal dander, causing moderate rhinoconjunctivitis and mild skin eruptions. No medication allergies are documented.\\n\\nClinical considerations:\\n- Hydrocodone/acetaminophen is not recommended during pregnancy due to potential risks of opioid exposure to the fetus. However, the prescription ended in September 2023, so unless the patient is still taking this medication, it should not impact current pregnancy planning.\\n- No other medications or medical conditions were identified that would directly impact fertility or pregnancy safety.\\n- No medication allergies are documented, but animal dander allergy is noted (not relevant to pregnancy).\\n\\nRecommendations:\\n- Confirm that the patient is not currently taking hydrocodone/acetaminophen or any other potentially teratogenic medications.\\n- If she is taking any other medications or has additional medical history not captured here, further review is warranted.\\n- Routine preconception counseling is advised, including folic acid supplementation, review of immunization status, and assessment for any chronic conditions not listed here.\\n\\nIf more detailed or recent medication or condition data is needed, please specify.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Test with both de-identification models separately\n",
    "test_patient_id = \"1329b83e-ea69-d184-b4af-0d2a8e07896e\"\n",
    "query = sample_queries[3]\n",
    "\n",
    "print(\"ðŸ” Testing Fine-tuned Gemma model:\")\n",
    "test_query(query, test_patient_id, \"finetuned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ” Testing Basic Gemma model:\")\n",
    "test_query(query, test_patient_id, \"basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
