{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df3e48d",
   "metadata": {},
   "source": [
    "# Privacy-Preserving RAG: Clinical Assistant with Tool Calling\n",
    "\n",
    "This notebook implements a clinical assistant powered by OpenAI's O3 model that can answer patient-specific questions using structured data while preserving privacy through local de-identification.\n",
    "\n",
    "## System Overview\n",
    "- **OpenAI O3 Agent**: Acts as orchestrator and QA engine\n",
    "- **Tool Functions**: Retrieve patient data from various sources\n",
    "- **Local De-identifier**: Removes PHI before sending to OpenAI\n",
    "- **Privacy-Safe Responses**: Clinically informed answers without compromising privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24c2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fetch_data import generate_note_auto;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978f1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.13/site-packages (1.98.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.13/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.13/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.13/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.13/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.13/site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.13/site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.13/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.13/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install essential packages\n",
    "# Run this cell first to install core dependencies\n",
    "\n",
    "!pip install openai python-dotenv pandas numpy \n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3be14cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d493c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.13/site-packages (0.34.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.13/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: llama-cpp-python in ./venv/lib/python3.13/site-packages (0.3.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (2.3.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./venv/lib/python3.13/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M2) - 10916 MiB free\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /Users/arunjoshi/.cache/huggingface/hub/models--google--gemma-3-4b-it-qat-q4_0-gguf/snapshots/15f73f5eee9c28f53afefef5723e29680c2fc78a/./gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
      "llama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\n",
      "llama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\n",
      "llama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\n",
      "llama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\n",
      "llama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\n",
      "llama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\n",
      "llama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\n",
      "llama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\n",
      "llama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\n",
      "llama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\n",
      "llama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\n",
      "llama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - type  f32:  205 tensors\n",
      "llama_model_loader: - type  f16:    1 tensors\n",
      "llama_model_loader: - type q4_0:  238 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 2.93 GiB (6.49 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token: 255999 '<start_of_image>' is not marked as EOG\n",
      "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token: 256000 '<end_of_image>' is not marked as EOG\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 1.9446 MB\n",
      "print_info: arch             = gemma3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 34\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 1024\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 10240\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 0.125\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 4B\n",
      "print_info: model params     = 3.88 B\n",
      "print_info: general.name     = n/a\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 262144\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 106 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 248 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 106 '<end_of_turn>'\n",
      "print_info: max token length = 93\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 206 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/35 layers to GPU\n",
      "load_tensors:   CPU_REPACK model buffer size =  1721.25 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  3002.65 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.28.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.28.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.29.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.30.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.31.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.32.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.32.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.33.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.33.ffn_down.weight with q4_0_4x8\n",
      "...\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 0.125\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x123e0de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x105ed8f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x123e0ff40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1232402f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x123240520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x105ed9140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x105ed9370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x105ed95a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x105ed97d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x12139fdc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x105ed9a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x105ed9c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x1213a01c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x123240750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x123240980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x123240bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x123e10170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x123240de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x123e104a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x123e106d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x123241010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x1213a03f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x123e10900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x123e10b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x123e10d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x123e10f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x1232a7930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x1213a0620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x1213a0850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x1213a0a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x1232a7b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x1213a0cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x14768e5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1213a0ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1232a7d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1213a1110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1213a1340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x1213a1570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123e111c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x123e113f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x105ed9e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1213a17a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1213a19d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1213a1c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123e11620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1232a7fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105eda090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105eda2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1232a8300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1232a8740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1232a8970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1213a1e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1213a2060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105eda4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1213a2290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15803c460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1213a24c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1232a8ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1213a26f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105eda720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x1213a2920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x1232a8dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x1213a2b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x1213a2d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x1232a9000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x1232a93d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x1213a2fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x1213a31e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x123e11850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x1213a3410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x1213a3640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1213a3870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x123e11a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1232a9670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1213a3aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x1213a3cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x105eda950 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x123e11cb0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1213a3f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x123e11ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123e12110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x1213a4130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123e12340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15803c690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1213a4360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105edab80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105edadb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1232a98a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1232a9ad0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1232a9d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1213a4590 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123e12570 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1213a47c0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123e127a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123e129d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105edafe0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105edb210 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1232a9f30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123e12c00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123e12e30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1232aa160 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123e13060 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1232aa390 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123e13290 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1232aa5c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123e134c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1213a49f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1213a4c20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123e136f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1213a4e50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1232aa7f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123e13920 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1232aaa20 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123e13b50 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1232aac50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1232aae80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123e13d80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1232ab0b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1213a5080 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1232ab2e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1213a52b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123e13fb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1213a54e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1213a5710 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123e141e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1213a5940 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123e14410 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1232ab510 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1232ab740 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1232ab970 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105edb440 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1232abba0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1213a5b70 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1232abdd0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1232ac000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123e14640 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1213a5da0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1232ac230 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1213a5fd0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123e14870 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12734ee00 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1232ac460 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105edb670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158149210 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123e14aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121b80730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123e14cd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121b80960 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1232ac690 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1232ac8c0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1232acaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123e14f00 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123e15130 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123e15360 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123e15590 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123e157c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12734f030 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105ed8c40 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12734f260 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12734f490 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12734f6c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123e159f0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12734f8f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1232acd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12734fcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123e15c20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123e15e50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123e16080 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127350100 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127350480 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123e162b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123e164e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1232acf50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1273506b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1232ad180 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1273508e0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127350b10 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127350d40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1232ad3b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127350f70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121b80c90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1273511a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1232ad5e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14768e910 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1273513d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1232ad810 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x121b80ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x1232ada40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x1232adc70 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x123e16710 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x121b810f0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x1232adea0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x123e16940 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x1232ae0d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x1232ae300 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x127351600 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x121b81320 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x1232ae530 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x121b81550 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x1232ae760 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x123e16b70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x121b81780 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x1273518a0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x127351ad0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x123e16da0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x121b819b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x121b81be0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x127351d00 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x123e16fd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x121b81e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x127351f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x121b82040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x123e17200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x121b82270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x127352160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x127352390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1273525c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1232ae990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x1273527f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x127352a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x127352c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123e174f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1232aebc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x127352e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x1232aedf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1232af020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1232af250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1273530b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1273532e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1232af480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123e17720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123e17950 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127353510 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1232af6b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1232af8e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123e17b80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x1232afb10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x123e0d760 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127353740 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x127353970 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127353ba0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123e17db0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127353dd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12323d410 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123e17fe0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x123e18210 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x1232afd40 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121b825a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x127354000 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121b827d0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123e18440 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123e18670 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1232aff70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127354230 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x1232b01a0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x121b82a00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127354460 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x1232b03d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127354690 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1273549a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127354bd0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127354e00 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127355030 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x1232b0600 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x123e188a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123e18ad0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x127355260 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127355490 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1232b0830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1232b0a60 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1273556c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1273558f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x127355d10 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x1232b0c90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123e18d00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x123e18f30 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127355f40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127356170 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1232b0ec0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1273563a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1273565d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x127356800 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x123e19160 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127356a30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x127356c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x1232b10f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x1232b1320 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x123e19390 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x127356e90 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x123e195c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x1232b1550 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x1273570c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x1273572f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x123e197f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x123e19a20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x121b82c30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x121b82e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121b83090 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121b832c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127357520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x127357750 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121b834f0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123e19c50 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x121b83720 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x121b83950 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x127357980 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x127357bb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x123e19e80 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x127357de0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x127358010 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x123e1a0b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x123e1a2e0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x121b83b80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x127358240 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x127358470 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123e1a510 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1273586a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1273588d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127358b00 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121b83db0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x127358d30 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x121b83fe0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x121b84210 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x127358f60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x121b84440 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x123e8c870 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x123e8caa0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x127359190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x123e8ccd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1273593c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1273595f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127359820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121b84670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127359a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123e8cf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127359c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1232b1780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127359eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123e8d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x123e8d360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1232b19b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12735a0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x123e8d620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1232b1be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1232b1e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1232b2040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1232b2270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x123e8da60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1232b24a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1232b26d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1232b2900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x123e8dc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x12735a310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x1232b2b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x121b848a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x123e8dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x121b84ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x121b84d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x1232b2d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x123e8e0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x121b84f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x121b85160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x12735a540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12735a770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123e8e320 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     1.00 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 2048 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified: layer  26: skipped\n",
      "llama_kv_cache_unified: layer  27: skipped\n",
      "llama_kv_cache_unified: layer  28: skipped\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: skipped\n",
      "llama_kv_cache_unified: layer  31: skipped\n",
      "llama_kv_cache_unified: layer  32: skipped\n",
      "llama_kv_cache_unified: layer  33: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    40.00 MiB\n",
      "llama_kv_cache_unified: size =   40.00 MiB (  2048 cells,   5 layers,  1 seqs), K (f16):   20.00 MiB, V (f16):   20.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 2048 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: skipped\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   232.00 MiB\n",
      "llama_kv_cache_unified: size =  232.00 MiB (  2048 cells,  29 layers,  1 seqs), K (f16):  116.00 MiB, V (f16):  116.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   522.00 MiB\n",
      "llama_context: graph nodes  = 1537\n",
      "llama_context: graph splits = 70 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.pre': 'default', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'gemma3.vision.num_channels': '3', 'gemma3.vision.image_size': '896', 'gemma3.vision.attention.head_count': '16', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2', 'gemma3.mm.tokens_per_image': '256', 'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.chat_template': '{{ bos_token }} {%- if messages[0][\\'role\\'] == \\'system\\' -%} {%- if messages[0][\\'content\\'] is string -%} {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\\\n\\' -%} {%- else -%} {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\\\n\\' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message[\\'role\\'] == \\'assistant\\') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message[\\'role\\'] -%} {%- endif -%} {{ \\'<start_of_turn>\\' + role + \\'\\\\n\\' + (first_user_prefix if loop.first else \"\") }} {%- if message[\\'content\\'] is string -%} {{ message[\\'content\\'] | trim }} {%- elif message[\\'content\\'] is iterable -%} {%- for item in message[\\'content\\'] -%} {%- if item[\\'type\\'] == \\'image\\' -%} {{ \\'<start_of_image>\\' }} {%- elif item[\\'type\\'] == \\'text\\' -%} {{ item[\\'text\\'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ \\'<end_of_turn>\\\\n\\' }} {%- endfor -%} {%- if add_generation_prompt -%} {{\\'<start_of_turn>model\\\\n\\'}} {%- endif -%}', 'general.quantization_version': '2', 'gemma3.attention.head_count_kv': '4', 'tokenizer.ggml.padding_token_id': '0', 'gemma3.attention.sliding_window': '1024', 'gemma3.rope.freq_base': '1000000.000000', 'gemma3.rope.scaling.factor': '8.000000', 'tokenizer.ggml.model': 'llama', 'gemma3.context_length': '131072', 'gemma3.vision.feed_forward_length': '4304', 'gemma3.rope.scaling.type': 'linear', 'gemma3.vision.attention.layer_norm_epsilon': '0.000001', 'tokenizer.ggml.unknown_token_id': '3', 'gemma3.embedding_length': '2560', 'gemma3.vision.block_count': '27', 'gemma3.attention.value_length': '256', 'gemma3.vision.embedding_length': '1152', 'general.file_type': '2', 'gemma3.vision.patch_size': '14', 'gemma3.block_count': '34', 'gemma3.attention.head_count': '8', 'gemma3.attention.key_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma3.feed_forward_length': '10240', 'general.architecture': 'gemma3'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }} {%- if messages[0]['role'] == 'system' -%} {%- if messages[0]['content'] is string -%} {%- set first_user_prefix = messages[0]['content'] + '\\n' -%} {%- else -%} {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message['role'] == 'assistant') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message['role'] -%} {%- endif -%} {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }} {%- if message['content'] is string -%} {{ message['content'] | trim }} {%- elif message['content'] is iterable -%} {%- for item in message['content'] -%} {%- if item['type'] == 'image' -%} {{ '<start_of_image>' }} {%- elif item['type'] == 'text' -%} {{ item['text'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ '<end_of_turn>\\n' }} {%- endfor -%} {%- if add_generation_prompt -%} {{'<start_of_turn>model\\n'}} {%- endif -%}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n"
     ]
    }
   ],
   "source": [
    "# gated model login with Hugging Face CLI\n",
    "# Make sure you have the Hugging Face CLI installed and authenticated\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "hugginfface_token = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "login(hugginfface_token)\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download the model to 'models/gemma-3-4b-it-qat-q4_0.gguf' as per previous steps\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-3-4b-it-qat-q4_0-gguf\",\n",
    "    filename=\"gemma-3-4b-it-q4_0.gguf\",  # ✅ correct filename\n",
    "    n_ctx=2048,\n",
    "    n_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d34b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated De-identification with Correct Context Window\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Roughly estimates token count, assuming 1 token ≈ 4 characters.\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "def merge_deidentified_chunks(chunks: List[str]) -> str:\n",
    "    \"\"\"Merges de-identified text chunks back into a single document.\"\"\"\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    return '\\n\\n'.join(chunks)\n",
    "\n",
    "# --- NEW: Token-based splitter ---\n",
    "def split_text_by_tokens(text: str, llm_model: Llama, max_tokens: int, overlap_tokens: int) -> List[str]:\n",
    "    \"\"\"Splits text into chunks of a specific token size using the model's tokenizer.\"\"\"\n",
    "    print(\".. using precise token-based splitter ..\")\n",
    "    \n",
    "    # 1. Tokenize the entire text using the model's tokenizer\n",
    "    tokens = llm_model.tokenize(text.encode(\"utf-8\", \"ignore\"))\n",
    "    \n",
    "    # 2. Create chunks of tokens\n",
    "    token_chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        token_chunks.append(tokens[start:end])\n",
    "        \n",
    "        # If this is the last chunk, we're done\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "            \n",
    "        # Move the start pointer back by the overlap amount, ensuring it doesn't get stuck\n",
    "        next_start = end - overlap_tokens\n",
    "        start = max(next_start, start + 1)\n",
    "\n",
    "    # 3. Convert token chunks back into text\n",
    "    text_chunks = [llm_model.detokenize(chunk).decode(\"utf-8\", \"ignore\") for chunk in token_chunks]\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b53de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt that instructs the model\n",
    "deidentifier_prompt = \"\"\"\n",
    "Remove all HIPAA-protected information and generate a concise clinical summary. Output only the de-identified summary, with no extra conversational text.\n",
    "\n",
    "Follow these rules strictly:\n",
    "1.  Remove all Protected Health Information (PHI) including: Names, Addresses/ZIP codes, All elements of dates except for the year, Phone/fax numbers, Email addresses, Social Security numbers, Medical record/account numbers, License numbers, Vehicle/device identifiers, URLs/IP addresses, Biometric identifiers, and Full-face photos.\n",
    "2.  Preserve all clinically relevant information.\n",
    "3.  Structure the output using clear headings and bullet points.\n",
    "4.  Maintain chronological order where applicable (most recent events first).\n",
    "\"\"\"\n",
    "\n",
    "# --- Model & Chunking Parameters ---\n",
    "# Set to the model's native context size\n",
    "MODEL_CONTEXT_WINDOW = 8192\n",
    "# Reserve tokens for the model's response\n",
    "MAX_OUTPUT_TOKENS = 2048\n",
    "# Small buffer for safety\n",
    "SAFETY_BUFFER_TOKENS = 100\n",
    "\n",
    "# --- Calculated Values (Do not change these directly) ---\n",
    "PROMPT_TOKENS = estimate_tokens(deidentifier_prompt)\n",
    "# Calculate the maximum number of tokens available for user input per chunk\n",
    "MAX_INPUT_TOKENS = MODEL_CONTEXT_WINDOW - PROMPT_TOKENS - MAX_OUTPUT_TOKENS - SAFETY_BUFFER_TOKENS\n",
    "# Convert the token limit to an approximate character limit for the splitter\n",
    "MAX_CHUNK_CHARACTERS = MAX_INPUT_TOKENS * 4\n",
    "# Character overlap between chunks to maintain context\n",
    "CHUNK_OVERLAP = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e925715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Authenticate and Load Model ---\n",
    "print(\"⚙️  Initializing model...\")\n",
    "\n",
    "# If you need to log in (usually only required once)\n",
    "# hf_token = os.getenv(\"HUGGING_FACE_API_KEY\")\n",
    "# login(token=hf_token)\n",
    "\n",
    "# Load the model from the Hugging Face Hub\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-3-4b-it-qat-q4_0-gguf\",\n",
    "    filename=\"gemma-3-4b-it-q4_0.gguf\",\n",
    "    n_ctx=MODEL_CONTEXT_WINDOW,  # Use the full 8K context\n",
    "    n_threads=8,                 # Adjust based on your CPU cores\n",
    "    verbose=False                # Set to True for detailed logs\n",
    ")\n",
    "\n",
    "print(\"✅ Model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentify_chunk(chunk: str) -> str:\n",
    "    \"\"\"Sends a single chunk to the LLM for de-identification.\"\"\"\n",
    "    try:\n",
    "        response = llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": deidentifier_prompt},\n",
    "                {\"role\": \"user\", \"content\": chunk}\n",
    "            ],\n",
    "            max_tokens=MAX_OUTPUT_TOKENS,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during chunk processing: {e}\")\n",
    "        return f\"[DEIDENTIFICATION_ERROR: {str(e)}]\"\n",
    "\n",
    "def deidentify(user_data: str) -> str:\n",
    "    \"\"\"Main function to de-identify text, using token-based chunking.\"\"\"\n",
    "    print(\"\\n🔍 Starting de-identification process...\")\n",
    "    # We use the model to get an exact token count\n",
    "    total_tokens = len(llm.tokenize(user_data.encode(\"utf-8\", \"ignore\")))\n",
    "    print(f\"📏 Input size: {len(user_data)} characters ({total_tokens} tokens)\")\n",
    "\n",
    "    # Check if the text is small enough to process in one go\n",
    "    if total_tokens <= MAX_INPUT_TOKENS:\n",
    "        print(\"✅ Text fits in context window, processing as a single chunk.\")\n",
    "        deidentified_text = deidentify_chunk(user_data)\n",
    "    else:\n",
    "        print(f\"📊 Text exceeds safe input size of {MAX_INPUT_TOKENS} tokens. Splitting into chunks...\")\n",
    "        \n",
    "        # --- USE THE NEW TOKEN SPLITTER ---\n",
    "        chunks = split_text_by_tokens(\n",
    "            user_data,\n",
    "            llm_model=llm,\n",
    "            max_tokens=MAX_INPUT_TOKENS,\n",
    "            overlap_tokens=CHUNK_OVERLAP\n",
    "        )\n",
    "        print(f\"🔧 Split into {len(chunks)} chunks.\") # This number will now be much smaller and correct\n",
    "\n",
    "        deidentified_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_token_count = len(llm.tokenize(chunk.encode(\"utf-8\", \"ignore\")))\n",
    "            print(f\"🔄 Processing chunk {i+1}/{len(chunks)} ({chunk_token_count} tokens)...\")\n",
    "            deidentified_chunk = deidentify_chunk(chunk)\n",
    "            deidentified_chunks.append(deidentified_chunk)\n",
    "\n",
    "        print(\"🔗 Merging de-identified chunks...\")\n",
    "        deidentified_text = merge_deidentified_chunks(deidentified_chunks)\n",
    "\n",
    "    print(f\"✅ De-identification complete. Output size: {len(deidentified_text)} characters.\")\n",
    "    return deidentified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0783a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DEIDENTIFICATION\n",
      "\n",
      "🔍 Starting de-identification process...\n",
      "📏 Input size: 1246 characters (558 tokens)\n",
      "✅ Text fits in context window, processing as a single chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Nirvana/masters/AI in healthcare/final_project/fetch_data.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_rows['date_only'] = pd.to_datetime(patient_rows['recorded_ts']).dt.date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ De-identification complete. Output size: 489 characters.\n",
      "original patient notes: \n",
      "📌 Patient Information (ID: e7a5d3dc-3484-a24e-6ef3-0737b403f950)\n",
      "- Full Name: Marni Tremblay\n",
      "- Birthdate: 1976-10-20\n",
      "- Ssn: 999-36-6337\n",
      "- Drivers: S99999779\n",
      "- Passport: X42697334X\n",
      "- Prefix: Mrs.\n",
      "- Marital: D\n",
      "- Race: white\n",
      "- Ethnicity: nonhispanic\n",
      "- Gender: F\n",
      "- Birthplace: Park River  North Dakota  US\n",
      "- Patient Address: 118 Kiehn Gardens Suite 40\n",
      "- Patient City: West Fargo\n",
      "- Patient State: North Dakota\n",
      "- County: Cass County\n",
      "- Fips: 38017.0\n",
      "- Patient Zip: 58078\n",
      "- Lat: 46.8934021332706\n",
      "- Lon: -96.8621390602698\n",
      "- Healthcare Expenses: 157011.5\n",
      "- Healthcare Coverage: 1055670.27\n",
      "- Income: 29501\n",
      "\n",
      "🧪 Observations:\n",
      "- [2025-04-12 09:00:18.000000]: Diastolic Blood Pressure (Code: 8462-4) = 61.0 mm[Hg] | Type: numeric | Category: vital-signs | Age: 48 | Provider: Otha Roberts (GENERAL PRACTICE) | Org: ESSENTIA HEALTH FARGO - WEST FARGO, ND | Encounter: Follow-up encounter (procedure) | Total Cost: $96.45 (Payer Coverage: $96.45)\n",
      "- [2025-04-12 09:00:18.000000]: Systolic Blood Pressure (Code: 8480-6) = 114.0 mm[Hg] | Type: numeric | Category: vital-signs | Age: 48 | Provider: Otha Roberts (GENERAL PRACTICE) | Org: ESSENTIA HEALTH FARGO - WEST FARGO, ND | Encounter: Follow-up encounter (procedure) | Total Cost: $96.45 (Payer Coverage: $96.45)\n",
      "Deidentified Notes: **Clinical Summary**\n",
      "\n",
      "*   **Age:** 48\n",
      "*   **Sex:** Female\n",
      "*   **Race:** White\n",
      "*   **Ethnicity:** Non-Hispanic\n",
      "*   **Vital Signs:**\n",
      "    *   Diastolic Blood Pressure: 61.0 mm[Hg]\n",
      "    *   Systolic Blood Pressure: 114.0 mm[Hg]\n",
      "*   **Provider:** Otha Roberts (GENERAL PRACTICE)\n",
      "*   **Organization:** ESSENTIA HEALTH FARGO - WEST FARGO, ND\n",
      "*   **Encounter Type:** Follow-up encounter (procedure)\n",
      "*   **Healthcare Expenses:** $96.45\n",
      "*   **Healthcare Coverage:** $96.45\n",
      "*   **Year of Birth:** 1976\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST DEIDENTIFICATION\")\n",
    "patient_notes = generate_note_auto(\"observations\", \"e7a5d3dc-3484-a24e-6ef3-0737b403f950\",)\n",
    "deidentified_notes = deidentify(patient_notes)\n",
    "print(\"original patient notes:\", patient_notes)\n",
    "print(\"Deidentified Notes:\", deidentified_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "267edf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI client initialized successfully!\n",
      "🤖 Model: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Configuration\n",
    "class ClinicalAssistantConfig:\n",
    "    \"\"\"Configuration for the Clinical Assistant\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # OpenAI API configuration\n",
    "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            print(\"⚠️  Warning: OPENAI_API_KEY not found in environment variables\")\n",
    "            print(\"💡 Please set your OpenAI API key in a .env file or environment variable\")\n",
    "        \n",
    "        # Model configuration\n",
    "        self.model = \"gpt-4.1\"  # Using GPT-4.1\n",
    "        self.max_completion_tokens = 4000  # Use max_completion_tokens for newer models\n",
    "        self.temperature = 0.1  # Low temperature for clinical accuracy\n",
    "        \n",
    "        # Tool calling configuration\n",
    "        self.max_tool_calls = 5\n",
    "        self.parallel_tool_calls = True\n",
    "\n",
    "# Initialize configuration\n",
    "config = ClinicalAssistantConfig()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if config.api_key:\n",
    "    client = OpenAI(api_key=config.api_key)\n",
    "    print(\"✅ OpenAI client initialized successfully!\")\n",
    "    print(f\"🤖 Model: {config.model}\")\n",
    "else:\n",
    "    client = None\n",
    "    print(\"❌ OpenAI client not initialized - API key required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75bc0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tool function definitions created!\n",
      "🔧 Available tools: 8\n",
      "   - get_patient_observations: Retrieve laboratory test results and clinical observations for a patient\n",
      "   - get_patient_conditions: Retrieve patient diagnosis information, medical conditions, and medical history\n",
      "   - get_patient_medications: Retrieve current and past medications for a patient\n",
      "   - get_patient_careplans: Retrieve care plans and treatment plans for a patient\n",
      "   - get_patient_procedures: Retrieve medical procedures and interventions performed on a patient\n",
      "   - get_patient_imaging_studies: Retrieve imaging studies and radiology reports for a patient\n",
      "   - get_patient_immunizations: Retrieve vaccination history and immunization records for a patient\n",
      "   - get_patient_allergies: Retrieve allergy information and adverse reactions for a patient\n"
     ]
    }
   ],
   "source": [
    "# Tool Function Definitions for OpenAI Function Calling\n",
    "# These will be used by the O3 model to understand available tools\n",
    "\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_observations\",\n",
    "            \"description\": \"Retrieve laboratory test results and clinical observations for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_conditions\",\n",
    "            \"description\": \"Retrieve patient diagnosis information, medical conditions, and medical history\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_medications\",\n",
    "            \"description\": \"Retrieve current and past medications for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_careplans\",\n",
    "            \"description\": \"Retrieve care plans and treatment plans for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_procedures\",\n",
    "            \"description\": \"Retrieve medical procedures and interventions performed on a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_imaging_studies\",\n",
    "            \"description\": \"Retrieve imaging studies and radiology reports for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_immunizations\",\n",
    "            \"description\": \"Retrieve vaccination history and immunization records for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_patient_allergies\",\n",
    "            \"description\": \"Retrieve allergy information and adverse reactions for a patient\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"patient_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique patient identifier\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"patient_id\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✅ Tool function definitions created!\")\n",
    "print(f\"🔧 Available tools: {len(TOOL_DEFINITIONS)}\")\n",
    "for tool in TOOL_DEFINITIONS:\n",
    "    print(f\"   - {tool['function']['name']}: {tool['function']['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b282f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Clinical Data Tools initialized with all available note types\n",
      "✅ Clinical tools instance created with all available note types!\n",
      "📋 Available note types: observations, conditions, medications, careplans, procedures, imaging_studies, immunizations, allergies\n"
     ]
    }
   ],
   "source": [
    "# Tool Function Implementations (Using generate_notes_for_type)\n",
    "class ClinicalDataTools:\n",
    "    \"\"\"Clinical data retrieval tools for the OpenAI assistant\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.de_identifier = None  # Will be initialized later\n",
    "        print(\"🔧 Clinical Data Tools initialized with all available note types\")\n",
    "    \n",
    "    def get_patient_observations(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve laboratory test results and clinical observations for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"observations\", patient_id)\n",
    "    \n",
    "    def get_patient_conditions(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve patient diagnosis information, medical conditions, and medical history\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"conditions\", patient_id)\n",
    "    \n",
    "    def get_patient_medications(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve current and past medications for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"medications\", patient_id)\n",
    "\n",
    "    def get_patient_careplans(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve care plans and treatment plans for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"careplans\", patient_id)\n",
    "\n",
    "    def get_patient_procedures(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve medical procedures and interventions performed on a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"procedures\", patient_id)\n",
    "\n",
    "    def get_patient_imaging_studies(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve imaging studies and radiology reports for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"imaging_studies\", patient_id)\n",
    "\n",
    "    def get_patient_immunizations(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve vaccination history and immunization records for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"immunizations\", patient_id)\n",
    "\n",
    "    def get_patient_allergies(self, patient_id: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve allergy information and adverse reactions for a patient\n",
    "        \n",
    "        Args:\n",
    "            patient_id: Unique patient identifier\n",
    "        \"\"\"\n",
    "        return generate_note_auto(\"allergies\", patient_id)\n",
    "\n",
    "# Initialize the tools\n",
    "clinical_tools = ClinicalDataTools()\n",
    "print(\"✅ Clinical tools instance created with all available note types!\")\n",
    "print(\"📋 Available note types: observations, conditions, medications, careplans, procedures, imaging_studies, immunizations, allergies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a63381ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Call and Prompt Management Functions\n",
    "def build_system_prompt() -> str:\n",
    "    \"\"\"Build the system prompt for the clinical assistant\"\"\"\n",
    "    return f\"\"\"You are an AI Clinical Reasoning Assistant with expertise in internal medicine. \n",
    "    Analyze clinical data and respond to patient-specific questions using structured reasoning.\n",
    "\n",
    "    Available Tools:\n",
    "    - get_patient_observations(): Laboratory test results and clinical observations\n",
    "    - get_patient_conditions(): Diagnosis history, medical conditions, and medical history\n",
    "    - get_patient_medications(): Current and past medications\n",
    "    - get_patient_careplans(): Care plans and treatment plans\n",
    "    - get_patient_procedures(): Medical procedures and interventions\n",
    "    - get_patient_imaging_studies(): Imaging studies and radiology reports\n",
    "    - get_patient_immunizations(): Vaccination history and immunization records\n",
    "    - get_patient_allergies(): Allergy information and adverse reactions\n",
    "\n",
    "    Clinical Reasoning Framework:\n",
    "    1. Analyze the question to determine needed information\n",
    "    2. Use appropriate tools to gather relevant patient data\n",
    "    3. Synthesize findings from multiple data sources\n",
    "    4. Provide clear, evidence-based responses with clinical reasoning\n",
    "\n",
    "    Rules:\n",
    "    - Use ONLY data provided by tool outputs\n",
    "    - Reference relative timeframes when provided (e.g., \"Day 0\", \"Post-op Day 3\")\n",
    "    - Acknowledge limitations if data is insufficient\n",
    "    - Suggest additional information needed when applicable\n",
    "    - Consider interactions between medications, conditions, and procedures\n",
    "    - Always check for allergies before recommending treatments\n",
    "\n",
    "    Current date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "    \"\"\"\n",
    "\n",
    "def build_messages(query: str, patient_id: str = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"Build messages for the LLM call\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": build_system_prompt()},\n",
    "        {\"role\": \"user\", \"content\": f\"Clinical query: {query}\"}\n",
    "    ]\n",
    "    \n",
    "    if patient_id:\n",
    "        messages[-1][\"content\"] += f\"\\\\nPatient ID: {patient_id}\"\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def call_llm(client: OpenAI, config: ClinicalAssistantConfig, messages: List[Dict[str, str]]):\n",
    "    \"\"\"Make the actual LLM call\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=config.model,\n",
    "        messages=messages,\n",
    "        tools=TOOL_DEFINITIONS,\n",
    "        tool_choice=\"auto\",\n",
    "        max_completion_tokens=config.max_completion_tokens,\n",
    "        temperature=config.temperature\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d447c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clinical Assistant initialized with all 8 tool functions!\n"
     ]
    }
   ],
   "source": [
    "# Clinical Assistant with Optional De-identification\n",
    "class ClinicalAssistant:\n",
    "    \"\"\"Main clinical assistant that orchestrates OpenAI GPT-4o-mini model with tool calling\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI, config: ClinicalAssistantConfig, tools: ClinicalDataTools):\n",
    "        self.client = client\n",
    "        self.config = config\n",
    "        self.tools = tools\n",
    "        \n",
    "        # Map function names to actual methods\n",
    "        self.function_map = {\n",
    "            \"get_patient_observations\": self.tools.get_patient_observations,\n",
    "            \"get_patient_conditions\": self.tools.get_patient_conditions,\n",
    "            \"get_patient_medications\": self.tools.get_patient_medications,\n",
    "            \"get_patient_careplans\": self.tools.get_patient_careplans,\n",
    "            \"get_patient_procedures\": self.tools.get_patient_procedures,\n",
    "            \"get_patient_imaging_studies\": self.tools.get_patient_imaging_studies,\n",
    "            \"get_patient_immunizations\": self.tools.get_patient_immunizations,\n",
    "            \"get_patient_allergies\": self.tools.get_patient_allergies\n",
    "        }\n",
    "    \n",
    "    def de_identify_data(self, data: str) -> str:\n",
    "        \"\"\"De-identify text data using the existing deidentify function\"\"\"\n",
    "        try:\n",
    "            return deidentify(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: De-identification failed: {e}\")\n",
    "            return data\n",
    "    \n",
    "    def execute_tool_call(self, function_name: str, arguments: Dict[str, Any], apply_deidentification: bool = False) -> str:\n",
    "        \"\"\"Execute a tool function call with optional de-identification\"\"\"\n",
    "        if function_name not in self.function_map:\n",
    "            return f\"Error: Unknown function: {function_name}\"\n",
    "        print(f\"Executing tool call: {function_name}\")\n",
    "        try:\n",
    "            func = self.function_map[function_name]\n",
    "            raw_result = func(**arguments)\n",
    "            \n",
    "            # Apply de-identification if requested\n",
    "            if apply_deidentification and isinstance(raw_result, str):\n",
    "                return self.de_identify_data(raw_result)\n",
    "            \n",
    "            return raw_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error executing {function_name}: {str(e)}\"\n",
    "    \n",
    "    def process_clinical_query(self, query: str, patient_id: str = None, apply_deidentification: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Process a clinical query using OpenAI GPT-4o-mini with tool calling\n",
    "        \n",
    "        Args:\n",
    "            query: The clinical question to answer\n",
    "            patient_id: Optional patient ID if known\n",
    "            apply_deidentification: Whether to apply de-identification to tool outputs\n",
    "            \n",
    "        Returns:\n",
    "            Clinical assistant response\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            return \"❌ OpenAI client not initialized. Please check your API key.\"\n",
    "        \n",
    "        try:\n",
    "            # Get initial response from LLM\n",
    "            messages = build_messages(query, patient_id)\n",
    "            response = call_llm(self.client, self.config, messages)\n",
    "            \n",
    "            # Handle tool calls if any\n",
    "            if response.choices[0].message.tool_calls:\n",
    "                current_messages = messages.copy()\n",
    "                current_messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response.choices[0].message.content,\n",
    "                    \"tool_calls\": response.choices[0].message.tool_calls\n",
    "                })\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in response.choices[0].message.tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments)\n",
    "                    \n",
    "                    # Execute tool call with optional de-identification\n",
    "                    tool_result = self.execute_tool_call(function_name, function_args, apply_deidentification)\n",
    "                    \n",
    "                    current_messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": tool_result\n",
    "                    })\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                final_response = call_llm(self.client, self.config, current_messages)\n",
    "                return final_response.choices[0].message.content\n",
    "            else:\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"❌ Error processing query: {str(e)}\"\n",
    "\n",
    "# Initialize the clinical assistant\n",
    "if client:\n",
    "    assistant = ClinicalAssistant(client, config, clinical_tools)\n",
    "    print(\"✅ Clinical Assistant initialized with all 8 tool functions!\")\n",
    "else:\n",
    "    assistant = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eefdcd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Clinical Assistant with Integrated De-identification\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Clinical Assistant with Integrated De-identification\n",
    "print(\"🧪 Testing Clinical Assistant with Integrated De-identification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample clinical queries to test the system with all available tools\n",
    "sample_queries = [\n",
    "    \"What are this patient's current lab abnormalities and what do they suggest clinically?\",\n",
    "    \"Can you summarize this patient's medical conditions and current medications?\",\n",
    "    \"What procedures has this patient undergone and what was the care plan?\",\n",
    "    \"Does this patient have any allergies I should be aware of before prescribing?\",\n",
    "    \"What is this patient's vaccination status and immunization history?\",\n",
    "    \"What imaging studies have been performed and what were the findings?\",\n",
    "    \"Based on all available data, what is the comprehensive clinical picture?\",\n",
    "    \"Are there any drug interactions or contraindications based on current medications and allergies?\"\n",
    "]\n",
    "\n",
    "# Enhanced interactive testing function with de-identification options\n",
    "def test_query(query_text: str, patient_id: str = \"12345678\", apply_deidentification: bool = True):\n",
    "    \"\"\"\n",
    "    Test function for interactive querying with de-identification options\n",
    "    \n",
    "    Args:\n",
    "        query_text: The clinical question to ask\n",
    "        patient_id: Patient identifier (default: \"12345678\")\n",
    "        apply_deidentification: Whether to apply de-identification (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        Assistant response\n",
    "    \"\"\"\n",
    "    if not assistant:\n",
    "        return \"❌ Clinical Assistant not initialized. Please set your OpenAI API key.\"\n",
    "    \n",
    "    print(f\"🔍 Patient ID: {patient_id}\")\n",
    "    print(f\"📝 Query: {query_text}\")\n",
    "    print(f\"🔒 De-identification: {'Enabled' if apply_deidentification else 'Disabled'}\")\n",
    "    print(\"\\n🤖 Assistant Response:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    response = assistant.process_clinical_query(query_text, patient_id, apply_deidentification)\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(response)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a13644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Patient ID: e7a5d3dc-3484-a24e-6ef3-0737b403f950\n",
      "📝 Query: What procedures has this patient undergone and what was the care plan?\n",
      "🔒 De-identification: Enabled\n",
      "\n",
      "🤖 Assistant Response:\n",
      "------------------------------------------------------------\n",
      "Executing tool call: get_patient_procedures\n",
      "\n",
      "🔍 Starting de-identification process...\n",
      "📏 Input size: 34292 characters (14873 tokens)\n",
      "📊 Text exceeds safe input size of 5866 tokens. Splitting into chunks...\n",
      "Warning: De-identification failed: name 'OVERLAP_TOKENS' is not defined\n",
      "Executing tool call: get_patient_careplans\n",
      "\n",
      "🔍 Starting de-identification process...\n",
      "📏 Input size: 2937 characters (1097 tokens)\n",
      "✅ Text fits in context window, processing as a single chunk.\n",
      "✅ De-identification complete. Output size: 956 characters.\n",
      "------------------------------------------------------------\n",
      "Here is a summary of the procedures and care plans for patient ID e7a5d3dc-3484-a24e-6ef3-0737b403f950:\n",
      "\n",
      "Procedures:\n",
      "- The patient has undergone a wide range of procedures, including:\n",
      "    - Measurement of respiratory function (for acute bronchitis)\n",
      "    - Multiple assessments: health and social care needs, anxiety, substance use, and domestic abuse\n",
      "    - Depression screenings (multiple occasions)\n",
      "    - Screening for drug abuse\n",
      "    - Insertion, replacement, and removal of intrauterine contraceptive devices\n",
      "    - Insertion of subcutaneous contraceptive\n",
      "    - Medication reconciliation\n",
      "    - Plain chest X-ray and bone immobilization (for rib fracture)\n",
      "    - Face mask application (for suspected COVID-19)\n",
      "    - Standard pregnancy tests and prenatal ultrasounds\n",
      "    - Passive immunization and physical examinations during pregnancy\n",
      "    - Prenatal laboratory screenings (blood group, hemogram, hepatitis B/C, HIV, chlamydia, gonorrhea, syphilis, rubella, varicella, TB, urine tests)\n",
      "    - Evaluation of uterine fundal height and auscultation of fetal heart (multiple prenatal visits)\n",
      "    - Fetal anatomy study, alpha-fetoprotein test, chromosomal aneuploidy screening\n",
      "    - Administration of Tdap vaccine during pregnancy\n",
      "\n",
      "Care Plans:\n",
      "- The patient has had several care plans, including:\n",
      "    - Lifestyle education for hypertension (November 2024)\n",
      "    - Routine antenatal care (December 2024 and ongoing)\n",
      "    - Clinical management plan for hyperlipidemia (November 2022)\n",
      "    - Emergency care plan for fracture of the rib (March–June 2020)\n",
      "    - Infectious disease care plans (May–June 2020)\n",
      "    - Respiratory therapy care plan (June–October 2016)\n",
      "    - Self-care interventions (historical)\n",
      "\n",
      "Summary:\n",
      "The patient has received comprehensive preventive, reproductive, and chronic disease management care, including regular prenatal care in the current pregnancy, management of hypertension and hyperlipidemia, and acute care for respiratory and musculoskeletal issues. The care plans have been tailored to address both acute and chronic conditions, as well as preventive health needs.\n",
      "\n",
      "If you need details on a specific procedure or care plan, please specify.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test with a sample patient ID (this will be de-identified)\n",
    "test_patient_id = \"e7a5d3dc-3484-a24e-6ef3-0737b403f950\"\n",
    "query = sample_queries[2]\n",
    "test_query(query, test_patient_id, apply_deidentification=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
