{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df3e48d",
   "metadata": {},
   "source": [
    "# Privacy-Preserving RAG: Clinical Assistant with Tool Calling\n",
    "\n",
    "This notebook implements a clinical assistant powered by OpenAI's GPT4.1 model that can answer patient-specific questions using structured data while preserving privacy through local de-identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3962aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_note_auto(table_name, patient_id):\n",
    "    \"\"\"\n",
    "    Reads the generated *_notes.csv file for the given table and returns \n",
    "    the first clinical note found for the provided patient_id.\n",
    "\n",
    "    Parameters:\n",
    "    - table_name: str (e.g., \"observations\", \"medications\")\n",
    "    - patient_id: str (the patient_id to search for)\n",
    "\n",
    "    Returns:\n",
    "    - str: the clinical note or a message if not found\n",
    "    \"\"\"\n",
    "    notes_file = os.path.join(\"data\", f\"{table_name}_notes.csv\")\n",
    "\n",
    "    if not os.path.exists(notes_file):\n",
    "        return f\"Note file not found: {notes_file}\"\n",
    "\n",
    "    df = pd.read_csv(notes_file)\n",
    "    \n",
    "    # Normalize column names just in case\n",
    "    df.columns = [col.strip().lower() for col in df.columns]\n",
    "\n",
    "    if \"patient_id\" not in df.columns or \"clinical_note\" not in df.columns:\n",
    "        return \"Expected columns not found in the CSV.\"\n",
    "\n",
    "    matching_notes = df[df[\"patient_id\"] == patient_id]\n",
    "\n",
    "    if matching_notes.empty:\n",
    "        return f\"No note found for patient_id: {patient_id}\"\n",
    "\n",
    "    return matching_notes.iloc[0][\"clinical_note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install essential packages\n",
    "# Run this cell first to install core dependencies\n",
    "\n",
    "!pip install openai python-dotenv pandas numpy\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import OpenAI\n",
    "\n",
    "# Environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÖ Current date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26651648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gated model login with Hugging Face CLI\n",
    "# Make sure you have the Hugging Face CLI installed and authenticated\n",
    "!pip install huggingface_hub\n",
    "!pip install llama-cpp-python\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(os.getenv(\"HUGGING_FACE_API_KEY\"))\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Download the model to 'models/gemma-3-4b-it-qat-q4_0.gguf'\n",
    "basic_deidentifier = Llama.from_pretrained(\n",
    "    repo_id=\"google/gemma-3-4b-it-qat-q4_0-gguf\",\n",
    "    filename=\"gemma-3-4b-it-q4_0.gguf\",  \n",
    "    n_ctx=2048,\n",
    "    n_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab564dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"google/gemma-2b\"\n",
    "LORA_MODEL_PATH = \"./gemma-deid-lora/checkpoint-60\"\n",
    "\n",
    "# Load tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA weights\n",
    "gemma_finetuned_model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "gemma_finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0634333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_hipaa_prompt(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the common HIPAA compliance prompt for de-identification.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt for de-identification\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "        You are a local language model responsible for enforcing HIPAA compliance by identifying and \n",
    "        removing all Protected Health Information (PHI) from clinical text and structured data before \n",
    "        it is passed to an external system. Your task is to remove all 18 identifiers defined under \n",
    "        HIPAA's Safe Harbor rule while preserving the clinical meaning of the data.\n",
    "\n",
    "        Redact all identifiers like names, dates, addresses, SSNs, etc. with placeholder [REDACTED], without summarizing or altering clinical facts.\n",
    "\n",
    "        ---\n",
    "        <data_with_phi>\n",
    "        {user_data}\n",
    "        </data_with_phi>\n",
    "        <data_hipaa_compliant>\n",
    "    \"\"\"\n",
    "\n",
    "def _extract_redacted_content(raw_output: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the redacted content from model output.\n",
    "    \n",
    "    Args:\n",
    "        raw_output: Raw output from the model\n",
    "        prompt: Original prompt used\n",
    "    \n",
    "    Returns:\n",
    "        Extracted redacted content\n",
    "    \"\"\"\n",
    "    if \"<data_hipaa_compliant>\" in raw_output:\n",
    "        redacted_part = raw_output.split(\"<data_hipaa_compliant>\")[-1]\n",
    "        redacted_part = redacted_part.split(\"</data_hipaa_compliant>\")[0].strip()\n",
    "    else:\n",
    "        # Fallback: remove the prompt from the beginning\n",
    "        redacted_part = raw_output[len(prompt):].strip()\n",
    "    \n",
    "    return redacted_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentify_with_basic_gemma(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    De-identify clinical text using the basic Gemma model via llama-cpp-python.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        De-identified text with PHI redacted\n",
    "    \"\"\"\n",
    "    prompt = _build_hipaa_prompt(user_data)\n",
    "    \n",
    "    try:\n",
    "        response = basic_deidentifier.create_chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": user_data}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        raw_output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        redacted_content = _extract_redacted_content(raw_output, prompt)\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"üìÑ Original note: {user_data}\")\n",
    "        print(\"üîí Redaction complete (Basic Gemma). PHI has been removed from the clinical note.\")\n",
    "        print(f\"üìÑ Redacted note: {redacted_content}\")\n",
    "        \n",
    "        return redacted_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during basic de-identification: {e}\")\n",
    "        return f\"[DEIDENTIFICATION_ERROR: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentify_with_finetuned_gemma(user_data: str) -> str:\n",
    "    \"\"\"\n",
    "    De-identify clinical text using the fine-tuned Gemma model with LoRA weights.\n",
    "    \n",
    "    Args:\n",
    "        user_data: Clinical text containing PHI\n",
    "    \n",
    "    Returns:\n",
    "        De-identified text with PHI redacted\n",
    "    \"\"\"\n",
    "    prompt = _build_hipaa_prompt(user_data)\n",
    "    \n",
    "    try:\n",
    "        device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = gemma_finetuned_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        raw_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        redacted_content = _extract_redacted_content(raw_output, prompt)\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"üìÑ Original note: {user_data}\")\n",
    "        print(\"üîí Redaction complete (Fine-tuned Gemma). PHI has been removed from the clinical note.\")\n",
    "        print(f\"üìÑ Redacted note: {redacted_content}\")\n",
    "        \n",
    "        return redacted_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during fine-tuned de-identification: {e}\")\n",
    "        return f\"[DEIDENTIFICATION_ERROR: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b282f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained Clinical Assistant with all configurations built-in\n",
    "class ClinicalAssistant:\n",
    "    \"\"\"Self-contained clinical assistant with built-in configurations and tools\"\"\"\n",
    "    \n",
    "    def __init__(self, deidentify_model: str = \"finetuned\"):\n",
    "        \"\"\"\n",
    "        Initialize the Clinical Assistant with built-in configurations\n",
    "        \n",
    "        Args:\n",
    "            deidentify_model: \"basic\" or \"finetuned\" for de-identification model\n",
    "        \"\"\"\n",
    "        self.deidentify_model = deidentify_model\n",
    "        \n",
    "        # Initialize OpenAI configuration\n",
    "        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        self.model = \"gpt-4.1\"\n",
    "        self.max_completion_tokens = 5000\n",
    "        self.temperature = 0.1\n",
    "        \n",
    "        # Tool definitions\n",
    "        self.tool_definitions = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_observations\",\n",
    "                    \"description\": \"Retrieve laboratory test results and clinical observations for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_conditions\",\n",
    "                    \"description\": \"Retrieve patient diagnosis information, medical conditions, and medical history\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_medications\",\n",
    "                    \"description\": \"Retrieve current and past medications for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_careplans\",\n",
    "                    \"description\": \"Retrieve care plans and treatment plans for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_procedures\",\n",
    "                    \"description\": \"Retrieve medical procedures and interventions performed on a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_imaging_studies\",\n",
    "                    \"description\": \"Retrieve imaging studies and radiology reports for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_immunizations\",\n",
    "                    \"description\": \"Retrieve vaccination history and immunization records for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_patient_allergies\",\n",
    "                    \"description\": \"Retrieve allergy information and adverse reactions for a patient\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"patient_id\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Unique patient identifier\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"patient_id\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Function mapping - directly use generate_note_auto with appropriate table names\n",
    "        self.function_map = {\n",
    "            \"get_patient_observations\": lambda patient_id: generate_note_auto(\"observations\", patient_id),\n",
    "            \"get_patient_conditions\": lambda patient_id: generate_note_auto(\"conditions\", patient_id),\n",
    "            \"get_patient_medications\": lambda patient_id: generate_note_auto(\"medications\", patient_id),\n",
    "            \"get_patient_careplans\": lambda patient_id: generate_note_auto(\"careplans\", patient_id),\n",
    "            \"get_patient_procedures\": lambda patient_id: generate_note_auto(\"procedures\", patient_id),\n",
    "            \"get_patient_imaging_studies\": lambda patient_id: generate_note_auto(\"imaging_studies\", patient_id),\n",
    "            \"get_patient_immunizations\": lambda patient_id: generate_note_auto(\"immunizations\", patient_id),\n",
    "            \"get_patient_allergies\": lambda patient_id: generate_note_auto(\"allergies\", patient_id)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Clinical Assistant initialized with {self.deidentify_model} de-identification model!\")\n",
    "    \n",
    "    def _build_system_prompt(self) -> str:\n",
    "        \"\"\"Build the system prompt for the clinical assistant\"\"\"\n",
    "        return f\"\"\"You are an AI Clinical Reasoning Assistant with expertise in internal medicine. \n",
    "        Analyze clinical data and respond to patient-specific questions using structured reasoning.\n",
    "\n",
    "        Available Tools:\n",
    "        - get_patient_observations(): Laboratory test results and clinical observations\n",
    "        - get_patient_conditions(): Diagnosis history, medical conditions, and medical history\n",
    "        - get_patient_medications(): Current and past medications\n",
    "        - get_patient_careplans(): Care plans and treatment plans\n",
    "        - get_patient_procedures(): Medical procedures and interventions\n",
    "        - get_patient_imaging_studies(): Imaging studies and radiology reports\n",
    "        - get_patient_immunizations(): Vaccination history and immunization records\n",
    "        - get_patient_allergies(): Allergy information and adverse reactions\n",
    "\n",
    "        Clinical Reasoning Framework:\n",
    "        1. Analyze the question to determine needed information\n",
    "        2. Use appropriate tools to gather relevant patient data\n",
    "        3. Synthesize findings from multiple data sources\n",
    "        4. Provide clear, evidence-based responses with clinical reasoning\n",
    "\n",
    "        Rules:\n",
    "        - Use ONLY data provided by tool outputs\n",
    "        - Reference relative timeframes when provided (e.g., \"Day 0\", \"Post-op Day 3\")\n",
    "        - Acknowledge limitations if data is insufficient\n",
    "        - Suggest additional information needed when applicable\n",
    "        - Consider interactions between medications, conditions, and procedures\n",
    "        - Always check for allergies before recommending treatments\n",
    "\n",
    "        Current date: {datetime.now().strftime('%Y-%m-%d')}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _de_identify_data(self, data: str) -> str:\n",
    "        \"\"\"De-identify text data using the specified deidentify model\"\"\"\n",
    "        if self.deidentify_model == \"basic\":\n",
    "            return deidentify_with_basic_gemma(data)\n",
    "        elif self.deidentify_model == \"finetuned\":\n",
    "            return deidentify_with_finetuned_gemma(data)        \n",
    "    \n",
    "    def _execute_tool_call(self, function_name: str, arguments: Dict[str, Any]) -> str:\n",
    "        \"\"\"Execute a tool function call with de-identification\"\"\"\n",
    "        if function_name not in self.function_map:\n",
    "            return f\"Error: Unknown function: {function_name}\"\n",
    "        \n",
    "        print(f\"Executing tool call: {function_name}\")\n",
    "        try:\n",
    "            func = self.function_map[function_name]\n",
    "            raw_result = func(**arguments)\n",
    "            return self._de_identify_data(raw_result)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {function_name}: {str(e)}\"\n",
    "\n",
    "    def process_query(self, query: str, patient_id: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Process a clinical query using OpenAI GPT with tool calling\n",
    "        \n",
    "        Args:\n",
    "            query: The clinical question to answer\n",
    "            patient_id: Optional patient ID if known\n",
    "            \n",
    "        Returns:\n",
    "            Clinical assistant response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build messages\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self._build_system_prompt()},\n",
    "                {\"role\": \"user\", \"content\": f\"Clinical query: {query}\"}\n",
    "            ]\n",
    "            \n",
    "            if patient_id:\n",
    "                messages[-1][\"content\"] += f\"\\nPatient ID: {patient_id}\"\n",
    "            \n",
    "            # Get initial response from LLM\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                tools=self.tool_definitions,\n",
    "                tool_choice=\"auto\",\n",
    "                max_completion_tokens=self.max_completion_tokens,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            \n",
    "            # Handle tool calls if any\n",
    "            if response.choices[0].message.tool_calls:\n",
    "                current_messages = messages.copy()\n",
    "                current_messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response.choices[0].message.content,\n",
    "                    \"tool_calls\": response.choices[0].message.tool_calls\n",
    "                })\n",
    "                \n",
    "                # Process each tool call\n",
    "                for tool_call in response.choices[0].message.tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_args = json.loads(tool_call.function.arguments)\n",
    "                    \n",
    "                    # Execute tool call with de-identification\n",
    "                    tool_result = self._execute_tool_call(function_name, function_args)\n",
    "                    \n",
    "                    current_messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": tool_result\n",
    "                    })\n",
    "                \n",
    "                # Get final response with tool results\n",
    "                final_response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=current_messages,\n",
    "                    tools=self.tool_definitions,\n",
    "                    tool_choice=\"auto\",\n",
    "                    max_completion_tokens=self.max_completion_tokens,\n",
    "                    temperature=self.temperature\n",
    "                )\n",
    "                return final_response.choices[0].message.content\n",
    "            else:\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error processing query: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Clinical Assistant with Different De-identification Models\n",
    "print(\"üß™ Testing Clinical Assistant with Integrated De-identification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample clinical queries to test the system with all available tools\n",
    "sample_queries = [\n",
    "    \"Can you summarize this patient's medical conditions and current medications?\",\n",
    "    \"Does this patient have any allergies I should be aware of before prescribing?\",\n",
    "    \"What is this patient's vaccination status and immunization history?\",\n",
    "    \"What imaging studies have been performed and what were the findings?\",\n",
    "]\n",
    "\n",
    "# Enhanced interactive testing function that creates assistant instances\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def test_query(query_text: str, patient_id: str = \"12345678\", deidentify_model: str = \"finetuned\"):\n",
    "    \"\"\"\n",
    "    Test function that creates a Clinical Assistant instance with the specified model\n",
    "    \n",
    "    Args:\n",
    "        query_text: The clinical question to ask\n",
    "        patient_id: Patient identifier (default: \"12345678\")\n",
    "        deidentify_model: \"finetuned\" or \"basic\" for de-identification model\n",
    "    \n",
    "    Returns:\n",
    "        Assistant response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create assistant instance with the specified de-identification model\n",
    "        assistant = ClinicalAssistant(deidentify_model=deidentify_model)\n",
    "        \n",
    "        model_name = \"Fine-tuned Gemma\" if deidentify_model == \"finetuned\" else \"Basic Gemma\"\n",
    "        header_md = f\"### ü©∫ Clinical Query: {query_text}\\n\\n**Patient ID:** {patient_id}\\n**De-identification Model:** {model_name}\"\n",
    "        \n",
    "        display(Markdown(header_md))\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"### ü§ñ Assistant Response\"))\n",
    "\n",
    "        response = assistant.process_query(query_text, patient_id)\n",
    "        display(Markdown(f\"```\\n{response}\\n```\"))\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        display(Markdown(error_msg))\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a13644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test with both de-identification models separately\n",
    "test_patient_id = \"1329b83e-ea69-d184-b4af-0d2a8e07896e\"\n",
    "query = sample_queries[1]\n",
    "\n",
    "print(\"üîç Testing Fine-tuned Gemma model:\")\n",
    "test_query(query, test_patient_id, \"finetuned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"üîç Testing Basic Gemma model:\")\n",
    "test_query(query, test_patient_id, \"basic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
